{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from random import choice\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, Adamax\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, TensorBoard, LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_list(img_dir, base_path):\n",
    "    img_dir = os.path.join(base_path, img_dir)\n",
    "    map_ = {class_name: en for en, class_name in enumerate(os.listdir(img_dir))}\n",
    "    labels = []\n",
    "    img_list = []\n",
    "    for class_name in os.listdir(img_dir):\n",
    "        class_dir = os.path.join(img_dir, class_name)\n",
    "        label = map_[class_name]\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            if not img_path.endswith('jpg') and not img_path.endswith('png') and not img_path.endswith('jpeg'):\n",
    "                continue\n",
    "            img_list.append(img_path)\n",
    "            labels.append(label)\n",
    "    return img_list, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha=0.4):\n",
    "\n",
    "    total_lenght = y_pred.shape.as_list()[-1]\n",
    "    anchor = y_pred[:, 0:int(total_lenght * 1 / 3)]\n",
    "    positive = y_pred[:, int(total_lenght * 1 / 3):int(total_lenght * 2 / 3)]\n",
    "    negative = y_pred[:, int(total_lenght * 2 / 3):int(total_lenght * 3 / 3)]\n",
    "\n",
    "    # distance between the anchor and the positive\n",
    "    pos_dist = K.sum(K.square(anchor - positive), axis=1)\n",
    "\n",
    "    # distance between the anchor and the negative\n",
    "    neg_dist = K.sum(K.square(anchor - negative), axis=1)\n",
    "\n",
    "    # compute loss\n",
    "    basic_loss = pos_dist - neg_dist + alphatorchvision\n",
    "    loss = K.maximum(basic_loss, 0.0)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, img_dir, base_dir, batch_size=32, dim=(224, 224), n_channels=3,\n",
    "                 n_classes=1, shuffle=True, image_dir='.', val=False):\n",
    "        self.dim = dim\n",
    "        self.image_dir = image_dir\n",
    "        self.batch_size = batch_size\n",
    "        img_ids, labels = get_label_list(img_dir, base_dir)\n",
    "        self.img_ids = img_ids\n",
    "        self.labels = labels\n",
    "        self.class_labels = defaultdict(list)\n",
    "        for label, img_path in zip(self.labels, self.img_ids):\n",
    "            self.class_labels[label].append(img_path)\n",
    "        self.label_names = list(self.class_labels.keys())\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        self.indexes = None\n",
    "        self.on_epoch_end()\n",
    "        self.val = val\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.img_ids) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        list_IDs_temp = [self.img_ids[k] for k in indexes]\n",
    "        labels = [self.labels[k] for k in indexes]\n",
    "        X, y = self.__data_generation(list_IDs_temp, labels)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.img_ids))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp, labels):\n",
    "        X_anchor = np.zeros((self.batch_size, *self.dim, self.n_channels))\n",
    "        X_neg = np.zeros((self.batch_size, *self.dim, self.n_channels))\n",
    "        X_pos = np.zeros((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.zeros((self.batch_size), dtype=float)\n",
    "        for i, (name, label) in enumerate(zip(list_IDs_temp, labels)):\n",
    "            img = self.get_image(name)\n",
    "            positive_path = choice(self.class_labels[label])\n",
    "            pos_img = self.get_image(positive_path)\n",
    "            labs = self.label_names.copy()\n",
    "            labs.remove(label)\n",
    "            neg_label = choice(labs)\n",
    "            neg_path = choice(self.class_labels[neg_label])\n",
    "            neg_img = self.get_image(neg_path)\n",
    "            X_anchor[i] = img\n",
    "            X_neg[i] = neg_img\n",
    "            X_pos[i] = pos_img\n",
    "\n",
    "        return [X_anchor, X_pos, X_neg], y\n",
    "\n",
    "    def get_image(self, name):\n",
    "        img = cv2.imread(name)[..., ::-1]\n",
    "        img = self.resize_pad(img, self.dim[0])\n",
    "        return img\n",
    "\n",
    "    @staticmethod\n",
    "    def resize_pad(im, desired_size=224):\n",
    "        old_size = im.shape[:2]  # old_size is in (height, width) format\n",
    "\n",
    "        ratio = float(desired_size) / max(old_size)\n",
    "        new_size = tuple([int(x * ratio) for x in old_size])\n",
    "\n",
    "        im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "\n",
    "        delta_w = desired_size - new_size[1]\n",
    "        delta_h = desired_size - new_size[0]\n",
    "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "\n",
    "        color = [0, 0, 0]\n",
    "        new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,\n",
    "                                    value=color)\n",
    "\n",
    "        return new_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_model(lr=0.0001, weights=None, predict=None, fine_tune=None):\n",
    "    input_1 = Input(shape=(None, None, 3))\n",
    "    input_2 = Input(shape=(None, None, 3))\n",
    "    input_3 = Input(shape=(None, None, 3))\n",
    "\n",
    "    x1 = tf.keras.applications.mobilenet_v2.preprocess_input(input_1)\n",
    "    x2 = tf.keras.applications.mobilenet_v2.preprocess_input(input_2)\n",
    "    x3 = tf.keras.applications.mobilenet_v2.preprocess_input(input_3)\n",
    "    base_model = tf.keras.applications.resnet.ResNet50(weights='imagenet',\n",
    "                                                       include_top=False,\n",
    "                                                       pooling='avg')\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x1 = base_model(x1)\n",
    "    x2 = base_model(x2)\n",
    "    x3 = base_model(x3)\n",
    "    layer_normalizer = tf.keras.layers.LayerNormalization(name='layer_normalization')\n",
    "\n",
    "    x1 = layer_normalizer(x1)\n",
    "    x2 = layer_normalizer(x2)\n",
    "    x3 = layer_normalizer(x3)\n",
    "\n",
    "    dense_1 = Dense(vec_dim, activation=\"linear\", name=\"dense_image_1\", use_bias=False)\n",
    "\n",
    "    x1 = dense_1(x1)\n",
    "    x2 = dense_1(x2)\n",
    "    x3 = dense_1(x3)\n",
    "    _norm = Lambda(lambda x: K.l2_normalize(x, axis=-1))\n",
    "\n",
    "    x1 = _norm(x1)\n",
    "    x2 = _norm(x2)\n",
    "    x3 = _norm(x3)\n",
    "    x = Concatenate(axis=-1)([x1, x2, x3])\n",
    "    model = Model([input_1, input_2, input_3], x)\n",
    "\n",
    "    model.compile(loss=triplet_loss, optimizer=Adamax(lr=1e-4)) #AVG(lr, momentum=0.9))\n",
    "    if fine_tune and predict:\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "    if weights is not None:\n",
    "        model.load_weights(weights)\n",
    "        print('model has been loaded!')\n",
    "    if fine_tune and not predict:\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_3 (TensorFl [(None, None, None,  0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_4 (TensorFl [(None, None, None,  0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_5 (TensorFl [(None, None, None,  0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_3 (TensorFlowOp [(None, None, None,  0           tf_op_layer_RealDiv_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_4 (TensorFlowOp [(None, None, None,  0           tf_op_layer_RealDiv_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_5 (TensorFlowOp [(None, None, None,  0           tf_op_layer_RealDiv_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "resnet50 (Functional)           (None, 2048)         23587712    tf_op_layer_Sub_3[0][0]          \n",
      "                                                                 tf_op_layer_Sub_4[0][0]          \n",
      "                                                                 tf_op_layer_Sub_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 2048)         4096        resnet50[0][0]                   \n",
      "                                                                 resnet50[1][0]                   \n",
      "                                                                 resnet50[2][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_image_1 (Dense)           (None, 128)          262144      layer_normalization[0][0]        \n",
      "                                                                 layer_normalization[1][0]        \n",
      "                                                                 layer_normalization[2][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 128)          0           dense_image_1[0][0]              \n",
      "                                                                 dense_image_1[1][0]              \n",
      "                                                                 dense_image_1[2][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 384)          0           lambda_1[0][0]                   \n",
      "                                                                 lambda_1[1][0]                   \n",
      "                                                                 lambda_1[2][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,853,952\n",
      "Trainable params: 266,240\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "18/19 [===========================>..] - ETA: 2s - loss: 0.3772WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "19/19 [==============================] - 45s 2s/step - loss: 0.3759\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 2/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "18/19 [===========================>..] - ETA: 2s - loss: 0.3413WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "19/19 [==============================] - 44s 2s/step - loss: 0.3373\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 3/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "18/19 [===========================>..] - ETA: 2s - loss: 0.2359WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "19/19 [==============================] - 46s 2s/step - loss: 0.2296\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 4/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "18/19 [===========================>..] - ETA: 2s - loss: 0.1786WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "19/19 [==============================] - 45s 2s/step - loss: 0.1708\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 5/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "18/19 [===========================>..] - ETA: 2s - loss: 0.1380WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "19/19 [==============================] - 44s 2s/step - loss: 0.1407\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 6/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "18/19 [===========================>..] - ETA: 2s - loss: 0.1304WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "19/19 [==============================] - 45s 2s/step - loss: 0.1263\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 7/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "18/19 [===========================>..] - ETA: 2s - loss: 0.1446WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "19/19 [==============================] - 46s 2s/step - loss: 0.1422\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 8/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/19 [===========================>..] - ETA: 2s - loss: 0.1313WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "19/19 [==============================] - 46s 2s/step - loss: 0.1295\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 9/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "18/19 [===========================>..] - ETA: 2s - loss: 0.0971WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "19/19 [==============================] - 45s 2s/step - loss: 0.1030\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 10/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "18/19 [===========================>..] - ETA: 2s - loss: 0.1220WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "19/19 [==============================] - 45s 2s/step - loss: 0.1193\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    weights = None  # \"resnet50_triplet.h5\"\n",
    "    file_path = 'resnet50_triplet.h5'\n",
    "    epochs = 10\n",
    "    vec_dim = 128\n",
    "    BATCH_SIZE = 16\n",
    "    model = image_model(lr=0.0001, weights=weights, fine_tune=False, predict=False)\n",
    "    train_get = DataGenerator(img_dir='cowsNose', base_dir='.', batch_size=BATCH_SIZE)\n",
    "#     valid_get = DataGenerator(img_dir='validation', base_dir='..', val=True, batch_size=4)\n",
    "\n",
    "    logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tbCallBack = TensorBoard(log_dir=logdir, histogram_freq=0, write_graph=True, write_images=True)\n",
    "    checkpointer = ModelCheckpoint(filepath=file_path, verbose=1, save_best_only=True, save_weights_only=True)\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(patience=2, factor=0.1, cooldown=0, verbose=1)\n",
    "#     lr_schedule = LearningRateScheduler(scheduler, verbose=1)\n",
    "    model.fit(train_get,\n",
    "              use_multiprocessing=True,\n",
    "              # validation_data=valid_get,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              workers=4,\n",
    "              steps_per_epoch=len(train_get),\n",
    "              # validation_steps=len(valid_get),\n",
    "#               callbacks=[lr_schedule])\n",
    "    model.save_weights(\"cowsResnet50.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
