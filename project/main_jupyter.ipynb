{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "\n",
    "import tensorboard_logger as tb_logger\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from util import TwoCropTransform, AverageMeter\n",
    "from util import adjust_learning_rate, warmup_learning_rate\n",
    "from util import set_optimizer, save_model\n",
    "from resnet_model import SupConResNet\n",
    "from contrastive_loss import SupConLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_option():\n",
    "    parser = argparse.ArgumentParser('argument for training')\n",
    "\n",
    "    parser.add_argument('--print_freq', type=int, default=10, help='print frequency')\n",
    "    parser.add_argument('--save_freq', type=int, default=25, help='save frequency')\n",
    "    parser.add_argument('--batch_size', type=int, default=2, help='batch_size')\n",
    "    parser.add_argument('--num_workers', type=int, default=1, help='num of workers to use')\n",
    "    parser.add_argument('--epochs', type=int, default=50, help='number of training epochs')\n",
    "\n",
    "    # optimization\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.05, help='learning rate')\n",
    "    parser.add_argument('--lr_decay_epochs', type=str, default='700,800,900', help='where to decay lr, can be a list')\n",
    "    parser.add_argument('--lr_decay_rate', type=float, default=0.1, help='decay rate for learning rate')\n",
    "    parser.add_argument('--weight_decay', type=float, default=1e-4, help='weight decay')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n",
    "\n",
    "    # model dataset\n",
    "    parser.add_argument('--model', type=str, default='resnet18')\n",
    "    parser.add_argument('--dataset', type=str, default='path', help='dataset')\n",
    "    parser.add_argument('--mean', default=(0, 0, 0), type=str, help='mean of dataset in path in form of str tuple')\n",
    "    parser.add_argument('--std', default=(1, 1, 1), type=str, help='std of dataset in path in form of str tuple')\n",
    "    parser.add_argument('--data_folder', type=str, default='/home/ivoks/Desktop/cowsNose', help='path to custom dataset')\n",
    "    parser.add_argument('--size', type=int, default=256, help='parameter for RandomResizedCrop')\n",
    "    parser.add_argument('--method', type=str, default='SupCon', help='choose method')\n",
    "    parser.add_argument('--temp', type=float, default=0.07, help='temperature for loss function')\n",
    "    parser.add_argument('--cosine', action='store_true', help='using cosine annealing. Learning Rate Scheduler!')\n",
    "    parser.add_argument('--warm', action='store_true', help='warm-up for large batch training')\n",
    "    parser.add_argument('--trial', type=str, default='0', help='id for recording multiple runs')\n",
    "\n",
    "    opt, unknown = parser.parse_known_args()\n",
    "\n",
    "    # set the path according to the environment\n",
    "    opt.model_path = './save/SupCon/{}_models'.format(opt.dataset)\n",
    "    opt.tb_path = './save/SupCon/{}_tensorboard'.format(opt.dataset)\n",
    "\n",
    "    iterations = opt.lr_decay_epochs.split(',')\n",
    "    opt.lr_decay_epochs = list([])\n",
    "    for it in iterations:\n",
    "        opt.lr_decay_epochs.append(int(it))\n",
    "\n",
    "    opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}_temp_{}_trial_{}'. \\\n",
    "        format(opt.method, opt.model, opt.learning_rate,\n",
    "               opt.weight_decay, opt.batch_size, opt.temp, opt.trial)\n",
    "\n",
    "    if opt.cosine:\n",
    "        opt.model_name = '{}_cosine'.format(opt.model_name)\n",
    "\n",
    "    # warm-up for large-batch training,\n",
    "#     if opt.batch_size > 256:\n",
    "#         opt.warm = True\n",
    "#     if opt.warm:\n",
    "#         opt.model_name = '{}_warm'.format(opt.model_name)\n",
    "#         opt.warmup_from = 0.01\n",
    "#         opt.warm_epochs = 10\n",
    "#         if opt.cosine:\n",
    "#             eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n",
    "#             opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (\n",
    "#                     1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2\n",
    "#         else:\n",
    "#             opt.warmup_to = opt.learning_rate\n",
    "\n",
    "    opt.tb_folder = os.path.join(opt.tb_path, opt.model_name)\n",
    "    if not os.path.isdir(opt.tb_folder):\n",
    "        os.makedirs(opt.tb_folder)\n",
    "\n",
    "    opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n",
    "    if not os.path.isdir(opt.save_folder):\n",
    "        os.makedirs(opt.save_folder)\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_loader(opt):\n",
    "    mean = opt.mean\n",
    "    std = opt.std\n",
    "    normalize = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=opt.size, scale=(0.8, 1.)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([\n",
    "            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
    "            transforms.RandomRotation(10)\n",
    "        ], p=0.8),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "#     train_transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         normalize,\n",
    "#     ])\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(root=opt.data_folder, transform=TwoCropTransform(train_transform))\n",
    "#     train_dataset = datasets.ImageFolder(root=opt.data_folder, transform=train_transform)\n",
    "    train_sampler = None\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=opt.batch_size, shuffle=(train_sampler is None),\n",
    "        num_workers=opt.num_workers, pin_memory=True, sampler=train_sampler)\n",
    "\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(opt):\n",
    "    model = SupConResNet(name=opt.model)\n",
    "    criterion = SupConLoss(temperature=opt.temp)\n",
    "    clss_criterion = torch.nn.CrossEntropyLoss()\n",
    "    # enable synchronized Batch Normalization\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        clss_criterion = clss_criterion.cuda()\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    return model, criterion, clss_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, clss_criterion, optimizer, epoch, opt):\n",
    "    \"\"\"one epoch training\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    class_losses = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    for idx, (images, labels) in enumerate(train_loader):\n",
    "#         print(labels)\n",
    "        images = torch.cat([images[0], images[1]], dim=0)\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda(non_blocking=True)\n",
    "            labels = labels.cuda(non_blocking=True)\n",
    "        bsz = labels.shape[0]\n",
    "        warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)\n",
    "        features, clss = model(images)\n",
    "        f1, f2 = torch.split(features, [bsz, bsz], dim=0)\n",
    "        features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\n",
    "        loss = criterion(features, labels)\n",
    "\n",
    "        \n",
    "        # classification loss\n",
    "#         print('images.shape:{}'.format(images.shape))\n",
    "#         print('labels.shape:{}'.format(labels.shape))\n",
    "#         print('features.shape:{}'.format(features.shape))\n",
    "#         print('clss.shape:{}'.format(clss.shape))\n",
    "\n",
    "        labels = torch.cat([labels, labels], dim=0)\n",
    "        class_loss = clss_criterion(clss, labels)\n",
    "        \n",
    "        # update metric\n",
    "        losses.update(loss.item(), bsz)\n",
    "        \n",
    "        class_losses.update(class_loss.item(), bsz)\n",
    "        \n",
    "        # SGD\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        class_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # print info\n",
    "        if (idx + 1) % opt.print_freq == 0:\n",
    "#             print('Train: [{0}][{1}/{2}]\\t'\n",
    "#                   'Batch-Time: {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "#                   'loss {loss.val:.3f} ({loss.avg:.3f})'.format(epoch, idx + 1, len(train_loader),\n",
    "#                                                                 batch_time=batch_time, loss=losses))\n",
    "            print('Train: [{0}][{1}/{2}]\\t'\n",
    "                  'Batch-Time: {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Total loss {loss:.3f} ({loss:.3f})\\t'\n",
    "                  'contractive loss {contraloss.val:.3f} ({contraloss.avg:.3f})\\t'\n",
    "                  'classification loss {class_loss.val:.3f} ({class_loss.avg:.3f})\\t'.format(epoch, idx + 1, len(train_loader),\n",
    "                                                                batch_time=batch_time, loss=losses.val+class_losses.val, \n",
    "                                                                 contraloss=losses, class_loss=class_losses))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    return losses.avg, class_losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [1][10/145]\tBatch-Time: 0.307 (0.810)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (1.153)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [1][20/145]\tBatch-Time: 0.307 (0.562)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.126)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [1][30/145]\tBatch-Time: 0.316 (0.481)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.117)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [1][40/145]\tBatch-Time: 0.310 (0.441)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (1.112)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [1][50/145]\tBatch-Time: 0.309 (0.415)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.109)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [1][60/145]\tBatch-Time: 0.310 (0.397)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.108)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [1][70/145]\tBatch-Time: 0.311 (0.386)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (1.106)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [1][80/145]\tBatch-Time: 0.310 (0.378)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.105)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [1][90/145]\tBatch-Time: 0.310 (0.370)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (1.105)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [1][100/145]\tBatch-Time: 0.310 (0.364)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.104)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [1][110/145]\tBatch-Time: 0.310 (0.359)\tTotal loss 5.703 (5.703)\tcontractive loss 1.098 (1.104)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [1][120/145]\tBatch-Time: 0.312 (0.355)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.103)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [1][130/145]\tBatch-Time: 0.312 (0.352)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.103)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [1][140/145]\tBatch-Time: 0.332 (0.350)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.102)\tclassification loss 4.605 (4.605)\t\n",
      "epoch 1, total time 51.44\n",
      "Train: [2][10/145]\tBatch-Time: 0.312 (0.352)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [2][20/145]\tBatch-Time: 0.312 (0.333)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [2][30/145]\tBatch-Time: 0.337 (0.327)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [2][40/145]\tBatch-Time: 0.336 (0.327)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [2][50/145]\tBatch-Time: 0.338 (0.327)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [2][60/145]\tBatch-Time: 0.350 (0.327)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [2][70/145]\tBatch-Time: 0.313 (0.326)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [2][80/145]\tBatch-Time: 0.313 (0.324)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (1.099)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [2][90/145]\tBatch-Time: 0.314 (0.323)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (1.099)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [2][100/145]\tBatch-Time: 0.313 (0.322)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [2][110/145]\tBatch-Time: 0.313 (0.322)\tTotal loss 5.704 (5.704)\tcontractive loss 1.098 (1.099)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [2][120/145]\tBatch-Time: 0.313 (0.321)\tTotal loss 5.703 (5.703)\tcontractive loss 1.098 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [2][130/145]\tBatch-Time: 0.313 (0.320)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [2][140/145]\tBatch-Time: 0.314 (0.320)\tTotal loss 5.705 (5.705)\tcontractive loss 1.099 (1.099)\tclassification loss 4.606 (4.605)\t\n",
      "epoch 2, total time 46.50\n",
      "Train: [3][10/145]\tBatch-Time: 0.320 (0.325)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [3][20/145]\tBatch-Time: 0.313 (0.322)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [3][30/145]\tBatch-Time: 0.316 (0.320)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [3][40/145]\tBatch-Time: 0.315 (0.319)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [3][50/145]\tBatch-Time: 0.315 (0.318)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [3][60/145]\tBatch-Time: 0.314 (0.318)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [3][70/145]\tBatch-Time: 0.315 (0.318)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (1.099)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [3][80/145]\tBatch-Time: 0.315 (0.318)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [3][90/145]\tBatch-Time: 0.315 (0.318)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [3][100/145]\tBatch-Time: 0.318 (0.318)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [3][110/145]\tBatch-Time: 0.322 (0.318)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [3][120/145]\tBatch-Time: 0.321 (0.318)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [3][130/145]\tBatch-Time: 0.316 (0.318)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [3][140/145]\tBatch-Time: 0.316 (0.318)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "epoch 3, total time 46.08\n",
      "Train: [4][10/145]\tBatch-Time: 0.333 (0.312)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [4][20/145]\tBatch-Time: 0.315 (0.316)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [4][30/145]\tBatch-Time: 0.335 (0.319)\tTotal loss 5.703 (5.703)\tcontractive loss 1.098 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [4][40/145]\tBatch-Time: 0.318 (0.320)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [4][50/145]\tBatch-Time: 0.316 (0.320)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [4][60/145]\tBatch-Time: 0.316 (0.319)\tTotal loss 5.704 (5.704)\tcontractive loss 1.098 (1.099)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [4][70/145]\tBatch-Time: 0.316 (0.319)\tTotal loss 5.704 (5.704)\tcontractive loss 1.098 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [4][80/145]\tBatch-Time: 0.316 (0.319)\tTotal loss 5.704 (5.704)\tcontractive loss 1.098 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [4][90/145]\tBatch-Time: 0.329 (0.319)\tTotal loss 5.703 (5.703)\tcontractive loss 1.098 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [4][100/145]\tBatch-Time: 0.343 (0.320)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [4][110/145]\tBatch-Time: 0.339 (0.321)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [4][120/145]\tBatch-Time: 0.320 (0.322)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [4][130/145]\tBatch-Time: 0.339 (0.323)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [4][140/145]\tBatch-Time: 0.318 (0.323)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (1.099)\tclassification loss 4.604 (4.605)\t\n",
      "epoch 4, total time 46.88\n",
      "Train: [5][10/145]\tBatch-Time: 0.338 (0.313)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [5][20/145]\tBatch-Time: 0.329 (0.326)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [5][30/145]\tBatch-Time: 0.343 (0.329)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [5][40/145]\tBatch-Time: 0.328 (0.328)\tTotal loss 5.703 (5.703)\tcontractive loss 1.098 (1.099)\tclassification loss 4.605 (4.605)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [5][50/145]\tBatch-Time: 0.339 (0.330)\tTotal loss 5.704 (5.704)\tcontractive loss 1.098 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [5][60/145]\tBatch-Time: 0.333 (0.330)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [5][70/145]\tBatch-Time: 0.317 (0.330)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [5][80/145]\tBatch-Time: 0.317 (0.328)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [5][90/145]\tBatch-Time: 0.317 (0.327)\tTotal loss 5.703 (5.703)\tcontractive loss 1.098 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [5][100/145]\tBatch-Time: 0.316 (0.326)\tTotal loss 5.703 (5.703)\tcontractive loss 1.098 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [5][110/145]\tBatch-Time: 0.316 (0.326)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (1.099)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [5][120/145]\tBatch-Time: 0.316 (0.325)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [5][130/145]\tBatch-Time: 0.317 (0.324)\tTotal loss 5.703 (5.703)\tcontractive loss 1.098 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [5][140/145]\tBatch-Time: 0.319 (0.324)\tTotal loss 5.704 (5.704)\tcontractive loss 1.098 (1.099)\tclassification loss 4.605 (4.605)\t\n",
      "epoch 5, total time 46.97\n",
      "Train: [6][10/145]\tBatch-Time: 0.317 (0.310)\tTotal loss 5.705 (5.705)\tcontractive loss 1.099 (1.099)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [6][20/145]\tBatch-Time: 0.317 (0.314)\tTotal loss 5.704 (5.704)\tcontractive loss 1.098 (1.098)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [6][30/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.703 (5.703)\tcontractive loss 1.098 (1.098)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [6][40/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.703 (5.703)\tcontractive loss 1.098 (1.098)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [6][50/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 5.703 (5.703)\tcontractive loss 1.098 (1.098)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [6][60/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.098)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [6][70/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (1.098)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [6][80/145]\tBatch-Time: 0.316 (0.317)\tTotal loss 5.703 (5.703)\tcontractive loss 1.098 (1.098)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [6][90/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 5.702 (5.702)\tcontractive loss 1.098 (1.098)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [6][100/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.098)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [6][110/145]\tBatch-Time: 0.317 (0.318)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.098)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [6][120/145]\tBatch-Time: 0.317 (0.318)\tTotal loss 5.704 (5.704)\tcontractive loss 1.098 (1.098)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [6][130/145]\tBatch-Time: 0.317 (0.318)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (1.098)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [6][140/145]\tBatch-Time: 0.317 (0.318)\tTotal loss 5.703 (5.703)\tcontractive loss 1.098 (1.098)\tclassification loss 4.605 (4.605)\t\n",
      "epoch 6, total time 46.07\n",
      "Train: [7][10/145]\tBatch-Time: 0.319 (0.306)\tTotal loss 5.704 (5.704)\tcontractive loss 1.098 (1.098)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [7][20/145]\tBatch-Time: 0.319 (0.312)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.098)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [7][30/145]\tBatch-Time: 0.318 (0.314)\tTotal loss 5.702 (5.702)\tcontractive loss 1.097 (1.098)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [7][40/145]\tBatch-Time: 0.319 (0.315)\tTotal loss 5.702 (5.702)\tcontractive loss 1.097 (1.098)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [7][50/145]\tBatch-Time: 0.333 (0.316)\tTotal loss 5.702 (5.702)\tcontractive loss 1.096 (1.098)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [7][60/145]\tBatch-Time: 0.318 (0.320)\tTotal loss 5.701 (5.701)\tcontractive loss 1.097 (1.098)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [7][70/145]\tBatch-Time: 0.319 (0.320)\tTotal loss 5.703 (5.703)\tcontractive loss 1.098 (1.098)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [7][80/145]\tBatch-Time: 0.319 (0.319)\tTotal loss 5.702 (5.702)\tcontractive loss 1.098 (1.098)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [7][90/145]\tBatch-Time: 0.319 (0.319)\tTotal loss 5.701 (5.701)\tcontractive loss 1.097 (1.098)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [7][100/145]\tBatch-Time: 0.319 (0.319)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.098)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [7][110/145]\tBatch-Time: 0.319 (0.319)\tTotal loss 5.704 (5.704)\tcontractive loss 1.099 (1.097)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [7][120/145]\tBatch-Time: 0.319 (0.319)\tTotal loss 5.705 (5.705)\tcontractive loss 1.100 (1.097)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [7][130/145]\tBatch-Time: 0.319 (0.319)\tTotal loss 5.703 (5.703)\tcontractive loss 1.097 (1.097)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [7][140/145]\tBatch-Time: 0.318 (0.319)\tTotal loss 5.691 (5.691)\tcontractive loss 1.085 (1.097)\tclassification loss 4.605 (4.605)\t\n",
      "epoch 7, total time 46.25\n",
      "Train: [8][10/145]\tBatch-Time: 0.318 (0.309)\tTotal loss 5.710 (5.710)\tcontractive loss 1.105 (1.090)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [8][20/145]\tBatch-Time: 0.319 (0.314)\tTotal loss 5.700 (5.700)\tcontractive loss 1.094 (1.093)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [8][30/145]\tBatch-Time: 0.319 (0.315)\tTotal loss 5.707 (5.707)\tcontractive loss 1.103 (1.094)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [8][40/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.653 (5.653)\tcontractive loss 1.048 (1.091)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [8][50/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.661 (5.661)\tcontractive loss 1.056 (1.087)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [8][60/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 5.729 (5.729)\tcontractive loss 1.124 (1.083)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [8][70/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.537 (5.537)\tcontractive loss 0.932 (1.078)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [8][80/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 5.181 (5.181)\tcontractive loss 0.577 (1.057)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [8][90/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 5.519 (5.519)\tcontractive loss 0.914 (1.045)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [8][100/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 4.922 (4.922)\tcontractive loss 0.317 (1.029)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [8][110/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 5.964 (5.964)\tcontractive loss 1.359 (1.000)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [8][120/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 5.550 (5.550)\tcontractive loss 0.945 (0.990)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [8][130/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 5.708 (5.708)\tcontractive loss 1.103 (1.023)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [8][140/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 5.721 (5.721)\tcontractive loss 1.116 (1.023)\tclassification loss 4.605 (4.605)\t\n",
      "epoch 8, total time 45.98\n",
      "Train: [9][10/145]\tBatch-Time: 0.317 (0.306)\tTotal loss 5.793 (5.793)\tcontractive loss 1.188 (1.060)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [9][20/145]\tBatch-Time: 0.318 (0.312)\tTotal loss 5.212 (5.212)\tcontractive loss 0.607 (0.974)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [9][30/145]\tBatch-Time: 0.318 (0.314)\tTotal loss 4.941 (4.941)\tcontractive loss 0.337 (0.923)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [9][40/145]\tBatch-Time: 0.318 (0.315)\tTotal loss 5.698 (5.698)\tcontractive loss 1.094 (0.851)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [9][50/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 5.275 (5.275)\tcontractive loss 0.670 (0.888)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [9][60/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 6.628 (6.628)\tcontractive loss 2.023 (0.949)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [9][70/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 5.888 (5.888)\tcontractive loss 1.283 (0.958)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [9][80/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.546 (5.546)\tcontractive loss 0.941 (0.964)\tclassification loss 4.606 (4.605)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [9][90/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.399 (5.399)\tcontractive loss 0.794 (0.964)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [9][100/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.949 (5.949)\tcontractive loss 1.344 (0.970)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [9][110/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.728 (5.728)\tcontractive loss 1.122 (0.941)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [9][120/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.338 (5.338)\tcontractive loss 0.733 (0.910)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [9][130/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.137 (5.137)\tcontractive loss 0.533 (0.932)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [9][140/145]\tBatch-Time: 0.319 (0.317)\tTotal loss 5.075 (5.075)\tcontractive loss 0.470 (0.926)\tclassification loss 4.605 (4.605)\t\n",
      "epoch 9, total time 46.03\n",
      "Train: [10][10/145]\tBatch-Time: 0.318 (0.309)\tTotal loss 6.124 (6.124)\tcontractive loss 1.520 (0.694)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [10][20/145]\tBatch-Time: 0.318 (0.313)\tTotal loss 5.668 (5.668)\tcontractive loss 1.063 (0.946)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [10][30/145]\tBatch-Time: 0.318 (0.315)\tTotal loss 5.259 (5.259)\tcontractive loss 0.654 (0.924)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [10][40/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 5.698 (5.698)\tcontractive loss 1.095 (0.951)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [10][50/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 5.282 (5.282)\tcontractive loss 0.678 (0.881)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [10][60/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 7.437 (7.437)\tcontractive loss 2.832 (0.868)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [10][70/145]\tBatch-Time: 0.319 (0.317)\tTotal loss 5.847 (5.847)\tcontractive loss 1.243 (0.846)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [10][80/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 4.727 (4.727)\tcontractive loss 0.122 (0.823)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [10][90/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.980 (5.980)\tcontractive loss 1.375 (0.819)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [10][100/145]\tBatch-Time: 0.318 (0.318)\tTotal loss 4.666 (4.666)\tcontractive loss 0.061 (0.856)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [10][110/145]\tBatch-Time: 0.318 (0.318)\tTotal loss 5.832 (5.832)\tcontractive loss 1.227 (0.867)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [10][120/145]\tBatch-Time: 0.318 (0.318)\tTotal loss 5.793 (5.793)\tcontractive loss 1.188 (0.853)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [10][130/145]\tBatch-Time: 0.318 (0.318)\tTotal loss 6.180 (6.180)\tcontractive loss 1.575 (0.856)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [10][140/145]\tBatch-Time: 0.318 (0.318)\tTotal loss 6.151 (6.151)\tcontractive loss 1.546 (0.855)\tclassification loss 4.605 (4.605)\t\n",
      "epoch 10, total time 46.08\n",
      "Train: [11][10/145]\tBatch-Time: 0.318 (0.304)\tTotal loss 5.671 (5.671)\tcontractive loss 1.067 (0.926)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [11][20/145]\tBatch-Time: 0.318 (0.312)\tTotal loss 5.660 (5.660)\tcontractive loss 1.054 (0.955)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [11][30/145]\tBatch-Time: 0.318 (0.314)\tTotal loss 4.912 (4.912)\tcontractive loss 0.307 (0.881)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [11][40/145]\tBatch-Time: 0.318 (0.315)\tTotal loss 5.653 (5.653)\tcontractive loss 1.048 (0.830)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [11][50/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 6.211 (6.211)\tcontractive loss 1.606 (0.784)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [11][60/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 4.628 (4.628)\tcontractive loss 0.023 (0.814)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [11][70/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.293 (5.293)\tcontractive loss 0.688 (0.834)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [11][80/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.656 (5.656)\tcontractive loss 1.051 (0.849)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [11][90/145]\tBatch-Time: 0.319 (0.317)\tTotal loss 5.504 (5.504)\tcontractive loss 0.900 (0.808)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [11][100/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.899 (5.899)\tcontractive loss 1.294 (0.791)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [11][110/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.897 (5.897)\tcontractive loss 1.293 (0.791)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [11][120/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 4.620 (4.620)\tcontractive loss 0.015 (0.771)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [11][130/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.731 (5.731)\tcontractive loss 1.126 (0.794)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [11][140/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.150 (5.150)\tcontractive loss 0.545 (0.810)\tclassification loss 4.605 (4.605)\t\n",
      "epoch 11, total time 46.03\n",
      "Train: [12][10/145]\tBatch-Time: 0.318 (0.303)\tTotal loss 5.757 (5.757)\tcontractive loss 1.153 (1.012)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [12][20/145]\tBatch-Time: 0.320 (0.311)\tTotal loss 5.776 (5.776)\tcontractive loss 1.171 (1.070)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [12][30/145]\tBatch-Time: 0.318 (0.313)\tTotal loss 5.862 (5.862)\tcontractive loss 1.258 (0.992)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [12][40/145]\tBatch-Time: 0.318 (0.315)\tTotal loss 5.031 (5.031)\tcontractive loss 0.426 (0.932)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [12][50/145]\tBatch-Time: 0.318 (0.315)\tTotal loss 4.912 (4.912)\tcontractive loss 0.307 (0.914)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [12][60/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.012 (5.012)\tcontractive loss 0.409 (0.853)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [12][70/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 8.736 (8.736)\tcontractive loss 4.130 (0.851)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [12][80/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 6.265 (6.265)\tcontractive loss 1.659 (0.817)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [12][90/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 4.994 (4.994)\tcontractive loss 0.389 (0.799)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [12][100/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 4.858 (4.858)\tcontractive loss 0.254 (0.800)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [12][110/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.130 (5.130)\tcontractive loss 0.525 (0.804)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [12][120/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 6.829 (6.829)\tcontractive loss 2.225 (0.840)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [12][130/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.813 (5.813)\tcontractive loss 1.208 (0.826)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [12][140/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.771 (5.771)\tcontractive loss 1.166 (0.839)\tclassification loss 4.605 (4.605)\t\n",
      "epoch 12, total time 46.01\n",
      "Train: [13][10/145]\tBatch-Time: 0.318 (0.303)\tTotal loss 4.816 (4.816)\tcontractive loss 0.211 (0.621)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [13][20/145]\tBatch-Time: 0.318 (0.311)\tTotal loss 4.652 (4.652)\tcontractive loss 0.047 (0.608)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [13][30/145]\tBatch-Time: 0.318 (0.313)\tTotal loss 6.216 (6.216)\tcontractive loss 1.611 (0.688)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [13][40/145]\tBatch-Time: 0.318 (0.314)\tTotal loss 5.610 (5.610)\tcontractive loss 1.006 (0.811)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [13][50/145]\tBatch-Time: 0.318 (0.315)\tTotal loss 5.304 (5.304)\tcontractive loss 0.699 (0.853)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [13][60/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 5.515 (5.515)\tcontractive loss 0.910 (0.870)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [13][70/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 4.959 (4.959)\tcontractive loss 0.354 (0.853)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [13][80/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 4.724 (4.724)\tcontractive loss 0.119 (0.834)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [13][90/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.730 (5.730)\tcontractive loss 1.126 (0.830)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [13][100/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.425 (5.425)\tcontractive loss 0.820 (0.838)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [13][110/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 4.766 (4.766)\tcontractive loss 0.162 (0.813)\tclassification loss 4.604 (4.605)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [13][120/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 4.707 (4.707)\tcontractive loss 0.101 (0.803)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [13][130/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.867 (5.867)\tcontractive loss 1.263 (0.810)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [13][140/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 4.749 (4.749)\tcontractive loss 0.145 (0.814)\tclassification loss 4.604 (4.605)\t\n",
      "epoch 13, total time 46.01\n",
      "Train: [14][10/145]\tBatch-Time: 0.318 (0.306)\tTotal loss 5.465 (5.465)\tcontractive loss 0.861 (0.914)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [14][20/145]\tBatch-Time: 0.318 (0.312)\tTotal loss 4.791 (4.791)\tcontractive loss 0.186 (0.862)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [14][30/145]\tBatch-Time: 0.318 (0.314)\tTotal loss 6.075 (6.075)\tcontractive loss 1.470 (0.780)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [14][40/145]\tBatch-Time: 0.318 (0.315)\tTotal loss 5.310 (5.310)\tcontractive loss 0.705 (0.733)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [14][50/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 5.603 (5.603)\tcontractive loss 0.998 (0.647)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [14][60/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 4.971 (4.971)\tcontractive loss 0.367 (0.611)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [14][70/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 4.798 (4.798)\tcontractive loss 0.193 (0.642)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [14][80/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.018 (5.018)\tcontractive loss 0.412 (0.699)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [14][90/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 4.951 (4.951)\tcontractive loss 0.345 (0.684)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [14][100/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 4.655 (4.655)\tcontractive loss 0.050 (0.687)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [14][110/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.384 (5.384)\tcontractive loss 0.779 (0.683)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [14][120/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 4.845 (4.845)\tcontractive loss 0.241 (0.680)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [14][130/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 4.630 (4.630)\tcontractive loss 0.025 (0.671)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [14][140/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 6.178 (6.178)\tcontractive loss 1.574 (0.688)\tclassification loss 4.604 (4.605)\t\n",
      "epoch 14, total time 46.02\n",
      "Train: [15][10/145]\tBatch-Time: 0.318 (0.314)\tTotal loss 4.927 (4.927)\tcontractive loss 0.322 (0.773)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [15][20/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 5.979 (5.979)\tcontractive loss 1.374 (0.935)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [15][30/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.846 (5.846)\tcontractive loss 1.241 (0.861)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [15][40/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.987 (5.987)\tcontractive loss 1.383 (0.878)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [15][50/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.069 (5.069)\tcontractive loss 0.464 (0.865)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [15][60/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.113 (5.113)\tcontractive loss 0.508 (0.839)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [15][70/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.063 (5.063)\tcontractive loss 0.458 (0.829)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [15][80/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 5.145 (5.145)\tcontractive loss 0.540 (0.789)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [15][90/145]\tBatch-Time: 0.318 (0.318)\tTotal loss 4.635 (4.635)\tcontractive loss 0.030 (0.780)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [15][100/145]\tBatch-Time: 0.318 (0.318)\tTotal loss 6.226 (6.226)\tcontractive loss 1.620 (0.769)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [15][110/145]\tBatch-Time: 0.318 (0.318)\tTotal loss 5.235 (5.235)\tcontractive loss 0.629 (0.723)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [15][120/145]\tBatch-Time: 0.318 (0.318)\tTotal loss 8.895 (8.895)\tcontractive loss 4.290 (0.742)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [15][130/145]\tBatch-Time: 0.318 (0.318)\tTotal loss 5.046 (5.046)\tcontractive loss 0.441 (0.748)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [15][140/145]\tBatch-Time: 0.318 (0.318)\tTotal loss 5.537 (5.537)\tcontractive loss 0.932 (0.756)\tclassification loss 4.605 (4.605)\t\n",
      "epoch 15, total time 46.07\n",
      "Train: [16][10/145]\tBatch-Time: 0.316 (0.308)\tTotal loss 5.573 (5.573)\tcontractive loss 0.968 (0.429)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [16][20/145]\tBatch-Time: 0.317 (0.312)\tTotal loss 5.403 (5.403)\tcontractive loss 0.798 (0.672)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [16][30/145]\tBatch-Time: 0.317 (0.314)\tTotal loss 5.125 (5.125)\tcontractive loss 0.520 (0.638)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [16][40/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 6.119 (6.119)\tcontractive loss 1.513 (0.676)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [16][50/145]\tBatch-Time: 0.316 (0.315)\tTotal loss 7.571 (7.571)\tcontractive loss 2.967 (0.744)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [16][60/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.664 (4.664)\tcontractive loss 0.058 (0.699)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [16][70/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.682 (4.682)\tcontractive loss 0.076 (0.683)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [16][80/145]\tBatch-Time: 0.339 (0.317)\tTotal loss 4.856 (4.856)\tcontractive loss 0.252 (0.674)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [16][90/145]\tBatch-Time: 0.319 (0.318)\tTotal loss 5.089 (5.089)\tcontractive loss 0.484 (0.636)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [16][100/145]\tBatch-Time: 0.335 (0.318)\tTotal loss 4.674 (4.674)\tcontractive loss 0.069 (0.669)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [16][110/145]\tBatch-Time: 0.332 (0.319)\tTotal loss 6.096 (6.096)\tcontractive loss 1.491 (0.712)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [16][120/145]\tBatch-Time: 0.316 (0.320)\tTotal loss 4.902 (4.902)\tcontractive loss 0.297 (0.696)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [16][130/145]\tBatch-Time: 0.316 (0.320)\tTotal loss 5.723 (5.723)\tcontractive loss 1.117 (0.714)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [16][140/145]\tBatch-Time: 0.335 (0.321)\tTotal loss 6.527 (6.527)\tcontractive loss 1.922 (0.720)\tclassification loss 4.605 (4.605)\t\n",
      "epoch 16, total time 46.54\n",
      "Train: [17][10/145]\tBatch-Time: 0.317 (0.312)\tTotal loss 5.421 (5.421)\tcontractive loss 0.816 (0.783)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [17][20/145]\tBatch-Time: 0.319 (0.319)\tTotal loss 4.749 (4.749)\tcontractive loss 0.143 (0.637)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [17][30/145]\tBatch-Time: 0.326 (0.319)\tTotal loss 4.774 (4.774)\tcontractive loss 0.169 (0.834)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [17][40/145]\tBatch-Time: 0.317 (0.320)\tTotal loss 4.753 (4.753)\tcontractive loss 0.148 (0.751)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [17][50/145]\tBatch-Time: 0.316 (0.319)\tTotal loss 5.853 (5.853)\tcontractive loss 1.247 (0.736)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [17][60/145]\tBatch-Time: 0.317 (0.319)\tTotal loss 5.830 (5.830)\tcontractive loss 1.225 (0.765)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [17][70/145]\tBatch-Time: 0.318 (0.319)\tTotal loss 6.208 (6.208)\tcontractive loss 1.603 (0.789)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [17][80/145]\tBatch-Time: 0.316 (0.318)\tTotal loss 4.711 (4.711)\tcontractive loss 0.106 (0.774)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [17][90/145]\tBatch-Time: 0.317 (0.318)\tTotal loss 5.044 (5.044)\tcontractive loss 0.439 (0.761)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [17][100/145]\tBatch-Time: 0.317 (0.318)\tTotal loss 5.703 (5.703)\tcontractive loss 1.098 (0.755)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [17][110/145]\tBatch-Time: 0.317 (0.318)\tTotal loss 4.637 (4.637)\tcontractive loss 0.031 (0.739)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [17][120/145]\tBatch-Time: 0.316 (0.318)\tTotal loss 5.184 (5.184)\tcontractive loss 0.579 (0.764)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [17][130/145]\tBatch-Time: 0.317 (0.318)\tTotal loss 5.326 (5.326)\tcontractive loss 0.721 (0.771)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [17][140/145]\tBatch-Time: 0.316 (0.318)\tTotal loss 5.751 (5.751)\tcontractive loss 1.145 (0.751)\tclassification loss 4.606 (4.605)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17, total time 46.06\n",
      "Train: [18][10/145]\tBatch-Time: 0.318 (0.304)\tTotal loss 4.764 (4.764)\tcontractive loss 0.160 (1.107)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [18][20/145]\tBatch-Time: 0.319 (0.310)\tTotal loss 5.710 (5.710)\tcontractive loss 1.105 (1.055)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [18][30/145]\tBatch-Time: 0.317 (0.313)\tTotal loss 5.748 (5.748)\tcontractive loss 1.143 (1.006)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [18][40/145]\tBatch-Time: 0.317 (0.314)\tTotal loss 5.445 (5.445)\tcontractive loss 0.841 (0.951)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [18][50/145]\tBatch-Time: 0.316 (0.314)\tTotal loss 5.115 (5.115)\tcontractive loss 0.509 (0.887)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [18][60/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 5.740 (5.740)\tcontractive loss 1.136 (0.961)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [18][70/145]\tBatch-Time: 0.316 (0.315)\tTotal loss 5.673 (5.673)\tcontractive loss 1.069 (0.982)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [18][80/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 5.724 (5.724)\tcontractive loss 1.120 (0.957)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [18][90/145]\tBatch-Time: 0.318 (0.315)\tTotal loss 5.485 (5.485)\tcontractive loss 0.881 (0.926)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [18][100/145]\tBatch-Time: 0.316 (0.315)\tTotal loss 5.941 (5.941)\tcontractive loss 1.336 (0.906)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [18][110/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 6.093 (6.093)\tcontractive loss 1.489 (0.888)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [18][120/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 5.961 (5.961)\tcontractive loss 1.355 (0.880)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [18][130/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.470 (5.470)\tcontractive loss 0.865 (0.870)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [18][140/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 6.700 (6.700)\tcontractive loss 2.095 (0.891)\tclassification loss 4.605 (4.605)\t\n",
      "epoch 18, total time 45.79\n",
      "Train: [19][10/145]\tBatch-Time: 0.317 (0.304)\tTotal loss 5.484 (5.484)\tcontractive loss 0.880 (0.726)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [19][20/145]\tBatch-Time: 0.316 (0.310)\tTotal loss 4.777 (4.777)\tcontractive loss 0.172 (0.797)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [19][30/145]\tBatch-Time: 0.316 (0.313)\tTotal loss 5.436 (5.436)\tcontractive loss 0.831 (0.644)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [19][40/145]\tBatch-Time: 0.317 (0.314)\tTotal loss 4.636 (4.636)\tcontractive loss 0.031 (0.633)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [19][50/145]\tBatch-Time: 0.317 (0.314)\tTotal loss 5.430 (5.430)\tcontractive loss 0.825 (0.708)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [19][60/145]\tBatch-Time: 0.316 (0.315)\tTotal loss 5.835 (5.835)\tcontractive loss 1.230 (0.686)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [19][70/145]\tBatch-Time: 0.316 (0.315)\tTotal loss 6.546 (6.546)\tcontractive loss 1.943 (0.687)\tclassification loss 4.603 (4.605)\t\n",
      "Train: [19][80/145]\tBatch-Time: 0.316 (0.315)\tTotal loss 4.739 (4.739)\tcontractive loss 0.135 (0.691)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [19][90/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 5.642 (5.642)\tcontractive loss 1.037 (0.715)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [19][100/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 4.648 (4.648)\tcontractive loss 0.042 (0.722)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [19][110/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.491 (5.491)\tcontractive loss 0.887 (0.712)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [19][120/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 5.593 (5.593)\tcontractive loss 0.988 (0.687)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [19][130/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 4.628 (4.628)\tcontractive loss 0.024 (0.686)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [19][140/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.955 (5.955)\tcontractive loss 1.350 (0.691)\tclassification loss 4.605 (4.605)\t\n",
      "epoch 19, total time 45.80\n",
      "Train: [20][10/145]\tBatch-Time: 0.317 (0.302)\tTotal loss 5.625 (5.625)\tcontractive loss 1.021 (1.141)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [20][20/145]\tBatch-Time: 0.317 (0.310)\tTotal loss 5.657 (5.657)\tcontractive loss 1.052 (0.912)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [20][30/145]\tBatch-Time: 0.317 (0.312)\tTotal loss 5.640 (5.640)\tcontractive loss 1.035 (0.859)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [20][40/145]\tBatch-Time: 0.316 (0.313)\tTotal loss 5.317 (5.317)\tcontractive loss 0.712 (0.830)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [20][50/145]\tBatch-Time: 0.316 (0.314)\tTotal loss 5.438 (5.438)\tcontractive loss 0.833 (0.867)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [20][60/145]\tBatch-Time: 0.317 (0.314)\tTotal loss 6.344 (6.344)\tcontractive loss 1.739 (0.893)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [20][70/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 4.653 (4.653)\tcontractive loss 0.049 (0.813)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [20][80/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 5.721 (5.721)\tcontractive loss 1.116 (0.800)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [20][90/145]\tBatch-Time: 0.316 (0.315)\tTotal loss 5.739 (5.739)\tcontractive loss 1.134 (0.832)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [20][100/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 5.837 (5.837)\tcontractive loss 1.232 (0.833)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [20][110/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 5.357 (5.357)\tcontractive loss 0.752 (0.818)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [20][120/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.915 (4.915)\tcontractive loss 0.309 (0.831)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [20][130/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.336 (5.336)\tcontractive loss 0.731 (0.824)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [20][140/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 4.760 (4.760)\tcontractive loss 0.156 (0.798)\tclassification loss 4.605 (4.605)\t\n",
      "epoch 20, total time 45.80\n",
      "Train: [21][10/145]\tBatch-Time: 0.316 (0.307)\tTotal loss 6.044 (6.044)\tcontractive loss 1.440 (0.968)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [21][20/145]\tBatch-Time: 0.317 (0.312)\tTotal loss 6.138 (6.138)\tcontractive loss 1.535 (0.895)\tclassification loss 4.603 (4.605)\t\n",
      "Train: [21][30/145]\tBatch-Time: 0.317 (0.313)\tTotal loss 5.685 (5.685)\tcontractive loss 1.079 (0.804)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [21][40/145]\tBatch-Time: 0.317 (0.314)\tTotal loss 5.297 (5.297)\tcontractive loss 0.692 (0.804)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [21][50/145]\tBatch-Time: 0.316 (0.315)\tTotal loss 4.681 (4.681)\tcontractive loss 0.077 (0.796)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [21][60/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 6.887 (6.887)\tcontractive loss 2.282 (0.782)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [21][70/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 5.056 (5.056)\tcontractive loss 0.451 (0.749)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [21][80/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 4.639 (4.639)\tcontractive loss 0.033 (0.763)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [21][90/145]\tBatch-Time: 0.315 (0.316)\tTotal loss 5.956 (5.956)\tcontractive loss 1.351 (0.739)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [21][100/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 5.324 (5.324)\tcontractive loss 0.720 (0.729)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [21][110/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.503 (5.503)\tcontractive loss 0.898 (0.725)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [21][120/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.706 (4.706)\tcontractive loss 0.102 (0.696)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [21][130/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 6.340 (6.340)\tcontractive loss 1.737 (0.679)\tclassification loss 4.603 (4.605)\t\n",
      "Train: [21][140/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.907 (5.907)\tcontractive loss 1.304 (0.718)\tclassification loss 4.604 (4.605)\t\n",
      "epoch 21, total time 45.84\n",
      "Train: [22][10/145]\tBatch-Time: 0.317 (0.308)\tTotal loss 5.810 (5.810)\tcontractive loss 1.205 (0.557)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [22][20/145]\tBatch-Time: 0.316 (0.312)\tTotal loss 4.882 (4.882)\tcontractive loss 0.277 (0.678)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [22][30/145]\tBatch-Time: 0.317 (0.314)\tTotal loss 4.836 (4.836)\tcontractive loss 0.233 (0.755)\tclassification loss 4.603 (4.605)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [22][40/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 4.924 (4.924)\tcontractive loss 0.319 (0.729)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [22][50/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 4.670 (4.670)\tcontractive loss 0.065 (0.742)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [22][60/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 6.037 (6.037)\tcontractive loss 1.434 (0.765)\tclassification loss 4.603 (4.605)\t\n",
      "Train: [22][70/145]\tBatch-Time: 0.316 (0.315)\tTotal loss 4.636 (4.636)\tcontractive loss 0.032 (0.731)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [22][80/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.981 (4.981)\tcontractive loss 0.375 (0.749)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [22][90/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.679 (4.679)\tcontractive loss 0.076 (0.757)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [22][100/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 6.156 (6.156)\tcontractive loss 1.552 (0.760)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [22][110/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 4.658 (4.658)\tcontractive loss 0.052 (0.752)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [22][120/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.719 (4.719)\tcontractive loss 0.114 (0.737)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [22][130/145]\tBatch-Time: 0.344 (0.317)\tTotal loss 5.314 (5.314)\tcontractive loss 0.708 (0.761)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [22][140/145]\tBatch-Time: 0.317 (0.318)\tTotal loss 4.803 (4.803)\tcontractive loss 0.198 (0.760)\tclassification loss 4.604 (4.605)\t\n",
      "epoch 22, total time 46.10\n",
      "Train: [23][10/145]\tBatch-Time: 0.317 (0.309)\tTotal loss 5.855 (5.855)\tcontractive loss 1.252 (0.807)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [23][20/145]\tBatch-Time: 0.317 (0.313)\tTotal loss 5.347 (5.347)\tcontractive loss 0.742 (0.750)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [23][30/145]\tBatch-Time: 0.317 (0.314)\tTotal loss 4.897 (4.897)\tcontractive loss 0.293 (0.626)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [23][40/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 6.465 (6.465)\tcontractive loss 1.859 (0.556)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [23][50/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 6.653 (6.653)\tcontractive loss 2.047 (0.523)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [23][60/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 4.852 (4.852)\tcontractive loss 0.247 (0.582)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [23][70/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.899 (4.899)\tcontractive loss 0.294 (0.593)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [23][80/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.710 (5.710)\tcontractive loss 1.106 (0.591)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [23][90/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.083 (5.083)\tcontractive loss 0.478 (0.573)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [23][100/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 4.676 (4.676)\tcontractive loss 0.071 (0.538)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [23][110/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 4.619 (4.619)\tcontractive loss 0.014 (0.558)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [23][120/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.969 (4.969)\tcontractive loss 0.364 (0.615)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [23][130/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.728 (4.728)\tcontractive loss 0.123 (0.617)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [23][140/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.309 (5.309)\tcontractive loss 0.705 (0.641)\tclassification loss 4.604 (4.605)\t\n",
      "epoch 23, total time 45.89\n",
      "Train: [24][10/145]\tBatch-Time: 0.317 (0.304)\tTotal loss 4.636 (4.636)\tcontractive loss 0.032 (0.469)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [24][20/145]\tBatch-Time: 0.317 (0.310)\tTotal loss 5.227 (5.227)\tcontractive loss 0.622 (0.665)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [24][30/145]\tBatch-Time: 0.318 (0.313)\tTotal loss 6.138 (6.138)\tcontractive loss 1.533 (0.708)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [24][40/145]\tBatch-Time: 0.317 (0.314)\tTotal loss 5.218 (5.218)\tcontractive loss 0.613 (0.680)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [24][50/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 4.721 (4.721)\tcontractive loss 0.115 (0.765)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [24][60/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 5.621 (5.621)\tcontractive loss 1.016 (0.747)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [24][70/145]\tBatch-Time: 0.316 (0.315)\tTotal loss 5.340 (5.340)\tcontractive loss 0.735 (0.761)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [24][80/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 5.913 (5.913)\tcontractive loss 1.309 (0.783)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [24][90/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.775 (5.775)\tcontractive loss 1.170 (0.804)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [24][100/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 6.193 (6.193)\tcontractive loss 1.589 (0.823)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [24][110/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 5.823 (5.823)\tcontractive loss 1.218 (0.808)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [24][120/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.028 (5.028)\tcontractive loss 0.423 (0.795)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [24][130/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 5.599 (5.599)\tcontractive loss 0.995 (0.780)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [24][140/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 4.653 (4.653)\tcontractive loss 0.047 (0.772)\tclassification loss 4.605 (4.605)\t\n",
      "epoch 24, total time 45.83\n",
      "Train: [25][10/145]\tBatch-Time: 0.316 (0.312)\tTotal loss 6.364 (6.364)\tcontractive loss 1.759 (0.806)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [25][20/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 6.750 (6.750)\tcontractive loss 2.145 (0.820)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [25][30/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 4.630 (4.630)\tcontractive loss 0.025 (0.654)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [25][40/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 4.636 (4.636)\tcontractive loss 0.032 (0.609)\tclassification loss 4.603 (4.605)\t\n",
      "Train: [25][50/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 4.789 (4.789)\tcontractive loss 0.185 (0.564)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [25][60/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 4.667 (4.667)\tcontractive loss 0.062 (0.577)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [25][70/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 4.659 (4.659)\tcontractive loss 0.054 (0.614)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [25][80/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.472 (5.472)\tcontractive loss 0.867 (0.603)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [25][90/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.625 (4.625)\tcontractive loss 0.019 (0.580)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [25][100/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 4.628 (4.628)\tcontractive loss 0.023 (0.569)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [25][110/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.645 (4.645)\tcontractive loss 0.039 (0.556)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [25][120/145]\tBatch-Time: 0.336 (0.317)\tTotal loss 4.708 (4.708)\tcontractive loss 0.106 (0.589)\tclassification loss 4.602 (4.605)\t\n",
      "Train: [25][130/145]\tBatch-Time: 0.324 (0.318)\tTotal loss 6.013 (6.013)\tcontractive loss 1.409 (0.603)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [25][140/145]\tBatch-Time: 0.317 (0.318)\tTotal loss 4.859 (4.859)\tcontractive loss 0.255 (0.599)\tclassification loss 4.604 (4.605)\t\n",
      "epoch 25, total time 46.07\n",
      "Train: [26][10/145]\tBatch-Time: 0.316 (0.305)\tTotal loss 4.652 (4.652)\tcontractive loss 0.048 (0.955)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [26][20/145]\tBatch-Time: 0.317 (0.311)\tTotal loss 5.938 (5.938)\tcontractive loss 1.334 (0.878)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [26][30/145]\tBatch-Time: 0.317 (0.313)\tTotal loss 4.699 (4.699)\tcontractive loss 0.094 (0.747)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [26][40/145]\tBatch-Time: 0.317 (0.314)\tTotal loss 4.902 (4.902)\tcontractive loss 0.297 (0.730)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [26][50/145]\tBatch-Time: 0.317 (0.314)\tTotal loss 6.588 (6.588)\tcontractive loss 1.983 (0.724)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [26][60/145]\tBatch-Time: 0.316 (0.315)\tTotal loss 5.855 (5.855)\tcontractive loss 1.251 (0.721)\tclassification loss 4.604 (4.604)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [26][70/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 6.137 (6.137)\tcontractive loss 1.532 (0.765)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [26][80/145]\tBatch-Time: 0.316 (0.315)\tTotal loss 5.757 (5.757)\tcontractive loss 1.153 (0.741)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [26][90/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 4.779 (4.779)\tcontractive loss 0.174 (0.723)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [26][100/145]\tBatch-Time: 0.316 (0.315)\tTotal loss 7.543 (7.543)\tcontractive loss 2.938 (0.743)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [26][110/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.669 (4.669)\tcontractive loss 0.064 (0.749)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [26][120/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.740 (4.740)\tcontractive loss 0.135 (0.743)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [26][130/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.481 (5.481)\tcontractive loss 0.877 (0.741)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [26][140/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 5.297 (5.297)\tcontractive loss 0.692 (0.729)\tclassification loss 4.605 (4.605)\t\n",
      "epoch 26, total time 45.80\n",
      "Train: [27][10/145]\tBatch-Time: 0.317 (0.302)\tTotal loss 5.599 (5.599)\tcontractive loss 0.994 (0.558)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [27][20/145]\tBatch-Time: 0.317 (0.309)\tTotal loss 5.296 (5.296)\tcontractive loss 0.690 (0.726)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [27][30/145]\tBatch-Time: 0.316 (0.312)\tTotal loss 6.434 (6.434)\tcontractive loss 1.830 (0.744)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [27][40/145]\tBatch-Time: 0.317 (0.313)\tTotal loss 5.567 (5.567)\tcontractive loss 0.962 (0.763)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [27][50/145]\tBatch-Time: 0.317 (0.314)\tTotal loss 5.722 (5.722)\tcontractive loss 1.119 (0.800)\tclassification loss 4.603 (4.605)\t\n",
      "Train: [27][60/145]\tBatch-Time: 0.316 (0.314)\tTotal loss 5.802 (5.802)\tcontractive loss 1.198 (0.809)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [27][70/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 6.457 (6.457)\tcontractive loss 1.851 (0.817)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [27][80/145]\tBatch-Time: 0.342 (0.316)\tTotal loss 4.657 (4.657)\tcontractive loss 0.053 (0.757)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [27][90/145]\tBatch-Time: 0.319 (0.317)\tTotal loss 6.056 (6.056)\tcontractive loss 1.452 (0.746)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [27][100/145]\tBatch-Time: 0.320 (0.318)\tTotal loss 6.755 (6.755)\tcontractive loss 2.151 (0.744)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [27][110/145]\tBatch-Time: 0.317 (0.318)\tTotal loss 4.668 (4.668)\tcontractive loss 0.062 (0.703)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [27][120/145]\tBatch-Time: 0.316 (0.318)\tTotal loss 4.786 (4.786)\tcontractive loss 0.181 (0.683)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [27][130/145]\tBatch-Time: 0.316 (0.318)\tTotal loss 6.052 (6.052)\tcontractive loss 1.449 (0.697)\tclassification loss 4.603 (4.605)\t\n",
      "Train: [27][140/145]\tBatch-Time: 0.317 (0.319)\tTotal loss 5.742 (5.742)\tcontractive loss 1.137 (0.698)\tclassification loss 4.605 (4.605)\t\n",
      "epoch 27, total time 46.26\n",
      "Train: [28][10/145]\tBatch-Time: 0.326 (0.320)\tTotal loss 6.379 (6.379)\tcontractive loss 1.775 (0.831)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [28][20/145]\tBatch-Time: 0.317 (0.319)\tTotal loss 5.195 (5.195)\tcontractive loss 0.590 (0.823)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [28][30/145]\tBatch-Time: 0.317 (0.319)\tTotal loss 5.174 (5.174)\tcontractive loss 0.569 (0.876)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [28][40/145]\tBatch-Time: 0.318 (0.319)\tTotal loss 4.680 (4.680)\tcontractive loss 0.075 (0.857)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [28][50/145]\tBatch-Time: 0.317 (0.319)\tTotal loss 5.113 (5.113)\tcontractive loss 0.510 (0.858)\tclassification loss 4.603 (4.605)\t\n",
      "Train: [28][60/145]\tBatch-Time: 0.317 (0.318)\tTotal loss 5.857 (5.857)\tcontractive loss 1.254 (0.811)\tclassification loss 4.603 (4.605)\t\n",
      "Train: [28][70/145]\tBatch-Time: 0.317 (0.318)\tTotal loss 5.487 (5.487)\tcontractive loss 0.883 (0.797)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [28][80/145]\tBatch-Time: 0.318 (0.318)\tTotal loss 5.501 (5.501)\tcontractive loss 0.896 (0.815)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [28][90/145]\tBatch-Time: 0.318 (0.318)\tTotal loss 5.777 (5.777)\tcontractive loss 1.172 (0.814)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [28][100/145]\tBatch-Time: 0.317 (0.318)\tTotal loss 5.865 (5.865)\tcontractive loss 1.260 (0.810)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [28][110/145]\tBatch-Time: 0.318 (0.318)\tTotal loss 4.720 (4.720)\tcontractive loss 0.115 (0.819)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [28][120/145]\tBatch-Time: 0.320 (0.318)\tTotal loss 4.672 (4.672)\tcontractive loss 0.067 (0.816)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [28][130/145]\tBatch-Time: 0.318 (0.318)\tTotal loss 6.192 (6.192)\tcontractive loss 1.586 (0.802)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [28][140/145]\tBatch-Time: 0.318 (0.318)\tTotal loss 6.308 (6.308)\tcontractive loss 1.703 (0.825)\tclassification loss 4.605 (4.605)\t\n",
      "epoch 28, total time 46.13\n",
      "Train: [29][10/145]\tBatch-Time: 0.318 (0.306)\tTotal loss 4.868 (4.868)\tcontractive loss 0.264 (0.686)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [29][20/145]\tBatch-Time: 0.318 (0.312)\tTotal loss 4.766 (4.766)\tcontractive loss 0.161 (0.637)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [29][30/145]\tBatch-Time: 0.319 (0.314)\tTotal loss 4.651 (4.651)\tcontractive loss 0.047 (0.659)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [29][40/145]\tBatch-Time: 0.318 (0.315)\tTotal loss 5.874 (5.874)\tcontractive loss 1.268 (0.755)\tclassification loss 4.606 (4.604)\t\n",
      "Train: [29][50/145]\tBatch-Time: 0.318 (0.315)\tTotal loss 5.626 (5.626)\tcontractive loss 1.021 (0.826)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [29][60/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 5.401 (5.401)\tcontractive loss 0.797 (0.802)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [29][70/145]\tBatch-Time: 0.320 (0.317)\tTotal loss 4.849 (4.849)\tcontractive loss 0.244 (0.841)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [29][80/145]\tBatch-Time: 0.319 (0.317)\tTotal loss 5.293 (5.293)\tcontractive loss 0.689 (0.826)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [29][90/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 4.769 (4.769)\tcontractive loss 0.164 (0.800)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [29][100/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 4.672 (4.672)\tcontractive loss 0.068 (0.772)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [29][110/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 4.719 (4.719)\tcontractive loss 0.113 (0.750)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [29][120/145]\tBatch-Time: 0.319 (0.317)\tTotal loss 4.618 (4.618)\tcontractive loss 0.013 (0.733)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [29][130/145]\tBatch-Time: 0.319 (0.317)\tTotal loss 4.627 (4.627)\tcontractive loss 0.022 (0.746)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [29][140/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.701 (5.701)\tcontractive loss 1.096 (0.725)\tclassification loss 4.605 (4.605)\t\n",
      "epoch 29, total time 46.00\n",
      "Train: [30][10/145]\tBatch-Time: 0.318 (0.312)\tTotal loss 4.650 (4.650)\tcontractive loss 0.044 (0.599)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [30][20/145]\tBatch-Time: 0.319 (0.315)\tTotal loss 5.317 (5.317)\tcontractive loss 0.713 (0.633)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [30][30/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 4.639 (4.639)\tcontractive loss 0.035 (0.553)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [30][40/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 4.914 (4.914)\tcontractive loss 0.310 (0.667)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [30][50/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.661 (5.661)\tcontractive loss 1.056 (0.653)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [30][60/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 4.689 (4.689)\tcontractive loss 0.085 (0.652)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [30][70/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 4.915 (4.915)\tcontractive loss 0.310 (0.703)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [30][80/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.152 (5.152)\tcontractive loss 0.548 (0.705)\tclassification loss 4.603 (4.605)\t\n",
      "Train: [30][90/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.948 (5.948)\tcontractive loss 1.344 (0.709)\tclassification loss 4.605 (4.605)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [30][100/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 4.651 (4.651)\tcontractive loss 0.046 (0.709)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [30][110/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 5.846 (5.846)\tcontractive loss 1.241 (0.718)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [30][120/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 4.818 (4.818)\tcontractive loss 0.215 (0.698)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [30][130/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 4.793 (4.793)\tcontractive loss 0.188 (0.696)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [30][140/145]\tBatch-Time: 0.319 (0.317)\tTotal loss 4.618 (4.618)\tcontractive loss 0.014 (0.678)\tclassification loss 4.604 (4.604)\t\n",
      "epoch 30, total time 46.04\n",
      "Train: [31][10/145]\tBatch-Time: 0.317 (0.305)\tTotal loss 4.650 (4.650)\tcontractive loss 0.045 (0.530)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [31][20/145]\tBatch-Time: 0.319 (0.312)\tTotal loss 4.638 (4.638)\tcontractive loss 0.033 (0.563)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [31][30/145]\tBatch-Time: 0.318 (0.314)\tTotal loss 4.885 (4.885)\tcontractive loss 0.283 (0.620)\tclassification loss 4.602 (4.604)\t\n",
      "Train: [31][40/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 5.761 (5.761)\tcontractive loss 1.157 (0.696)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [31][50/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 5.831 (5.831)\tcontractive loss 1.225 (0.722)\tclassification loss 4.606 (4.604)\t\n",
      "Train: [31][60/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 5.889 (5.889)\tcontractive loss 1.284 (0.738)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [31][70/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 5.854 (5.854)\tcontractive loss 1.251 (0.745)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [31][80/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.677 (4.677)\tcontractive loss 0.072 (0.717)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [31][90/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 5.745 (5.745)\tcontractive loss 1.142 (0.709)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [31][100/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 6.928 (6.928)\tcontractive loss 2.323 (0.767)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [31][110/145]\tBatch-Time: 0.316 (0.317)\tTotal loss 4.767 (4.767)\tcontractive loss 0.162 (0.794)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [31][120/145]\tBatch-Time: 0.347 (0.318)\tTotal loss 4.836 (4.836)\tcontractive loss 0.233 (0.789)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [31][130/145]\tBatch-Time: 0.316 (0.318)\tTotal loss 5.103 (5.103)\tcontractive loss 0.498 (0.777)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [31][140/145]\tBatch-Time: 0.317 (0.318)\tTotal loss 4.758 (4.758)\tcontractive loss 0.153 (0.779)\tclassification loss 4.605 (4.604)\t\n",
      "epoch 31, total time 46.11\n",
      "Train: [32][10/145]\tBatch-Time: 0.317 (0.308)\tTotal loss 5.982 (5.982)\tcontractive loss 1.377 (0.692)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [32][20/145]\tBatch-Time: 0.316 (0.313)\tTotal loss 4.979 (4.979)\tcontractive loss 0.376 (0.540)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [32][30/145]\tBatch-Time: 0.317 (0.314)\tTotal loss 5.589 (5.589)\tcontractive loss 0.985 (0.521)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [32][40/145]\tBatch-Time: 0.316 (0.315)\tTotal loss 8.699 (8.699)\tcontractive loss 4.094 (0.557)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [32][50/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 4.718 (4.718)\tcontractive loss 0.112 (0.523)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [32][60/145]\tBatch-Time: 0.316 (0.315)\tTotal loss 5.230 (5.230)\tcontractive loss 0.626 (0.547)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [32][70/145]\tBatch-Time: 0.323 (0.317)\tTotal loss 6.388 (6.388)\tcontractive loss 1.784 (0.553)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [32][80/145]\tBatch-Time: 0.331 (0.318)\tTotal loss 6.085 (6.085)\tcontractive loss 1.481 (0.559)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [32][90/145]\tBatch-Time: 0.341 (0.319)\tTotal loss 4.651 (4.651)\tcontractive loss 0.046 (0.568)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [32][100/145]\tBatch-Time: 0.330 (0.320)\tTotal loss 4.996 (4.996)\tcontractive loss 0.391 (0.609)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [32][110/145]\tBatch-Time: 0.317 (0.320)\tTotal loss 4.762 (4.762)\tcontractive loss 0.156 (0.612)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [32][120/145]\tBatch-Time: 0.316 (0.320)\tTotal loss 5.171 (5.171)\tcontractive loss 0.567 (0.616)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [32][130/145]\tBatch-Time: 0.316 (0.320)\tTotal loss 5.660 (5.660)\tcontractive loss 1.056 (0.603)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [32][140/145]\tBatch-Time: 0.317 (0.320)\tTotal loss 4.731 (4.731)\tcontractive loss 0.127 (0.576)\tclassification loss 4.604 (4.604)\t\n",
      "epoch 32, total time 46.32\n",
      "Train: [33][10/145]\tBatch-Time: 0.316 (0.311)\tTotal loss 4.621 (4.621)\tcontractive loss 0.015 (0.783)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [33][20/145]\tBatch-Time: 0.316 (0.314)\tTotal loss 6.495 (6.495)\tcontractive loss 1.890 (0.666)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [33][30/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 4.734 (4.734)\tcontractive loss 0.129 (0.753)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [33][40/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 4.856 (4.856)\tcontractive loss 0.251 (0.758)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [33][50/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 5.203 (5.203)\tcontractive loss 0.600 (0.754)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [33][60/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.715 (5.715)\tcontractive loss 1.111 (0.766)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [33][70/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 6.014 (6.014)\tcontractive loss 1.410 (0.750)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [33][80/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.897 (5.897)\tcontractive loss 1.294 (0.760)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [33][90/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.780 (4.780)\tcontractive loss 0.174 (0.724)\tclassification loss 4.606 (4.604)\t\n",
      "Train: [33][100/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 6.180 (6.180)\tcontractive loss 1.574 (0.733)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [33][110/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.751 (4.751)\tcontractive loss 0.148 (0.735)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [33][120/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.668 (4.668)\tcontractive loss 0.064 (0.718)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [33][130/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.735 (4.735)\tcontractive loss 0.129 (0.702)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [33][140/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 6.552 (6.552)\tcontractive loss 1.948 (0.715)\tclassification loss 4.604 (4.604)\t\n",
      "epoch 33, total time 45.86\n",
      "Train: [34][10/145]\tBatch-Time: 0.317 (0.307)\tTotal loss 4.948 (4.948)\tcontractive loss 0.343 (0.745)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [34][20/145]\tBatch-Time: 0.317 (0.312)\tTotal loss 5.287 (5.287)\tcontractive loss 0.683 (0.664)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [34][30/145]\tBatch-Time: 0.316 (0.314)\tTotal loss 7.058 (7.058)\tcontractive loss 2.453 (0.660)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [34][40/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 5.708 (5.708)\tcontractive loss 1.103 (0.783)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [34][50/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 6.034 (6.034)\tcontractive loss 1.429 (0.844)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [34][60/145]\tBatch-Time: 0.321 (0.318)\tTotal loss 4.809 (4.809)\tcontractive loss 0.203 (0.781)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [34][70/145]\tBatch-Time: 0.316 (0.318)\tTotal loss 5.566 (5.566)\tcontractive loss 0.960 (0.815)\tclassification loss 4.606 (4.604)\t\n",
      "Train: [34][80/145]\tBatch-Time: 0.317 (0.318)\tTotal loss 5.864 (5.864)\tcontractive loss 1.261 (0.789)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [34][90/145]\tBatch-Time: 0.317 (0.318)\tTotal loss 4.791 (4.791)\tcontractive loss 0.187 (0.804)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [34][100/145]\tBatch-Time: 0.316 (0.318)\tTotal loss 4.677 (4.677)\tcontractive loss 0.072 (0.780)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [34][110/145]\tBatch-Time: 0.316 (0.318)\tTotal loss 4.714 (4.714)\tcontractive loss 0.111 (0.746)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [34][120/145]\tBatch-Time: 0.317 (0.318)\tTotal loss 5.242 (5.242)\tcontractive loss 0.637 (0.738)\tclassification loss 4.605 (4.604)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [34][130/145]\tBatch-Time: 0.317 (0.318)\tTotal loss 4.770 (4.770)\tcontractive loss 0.166 (0.721)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [34][140/145]\tBatch-Time: 0.317 (0.318)\tTotal loss 5.543 (5.543)\tcontractive loss 0.940 (0.730)\tclassification loss 4.602 (4.604)\t\n",
      "epoch 34, total time 46.08\n",
      "Train: [35][10/145]\tBatch-Time: 0.316 (0.307)\tTotal loss 6.305 (6.305)\tcontractive loss 1.701 (0.502)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [35][20/145]\tBatch-Time: 0.316 (0.313)\tTotal loss 4.688 (4.688)\tcontractive loss 0.083 (0.601)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [35][30/145]\tBatch-Time: 0.316 (0.314)\tTotal loss 4.724 (4.724)\tcontractive loss 0.120 (0.602)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [35][40/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 4.678 (4.678)\tcontractive loss 0.076 (0.666)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [35][50/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 5.822 (5.822)\tcontractive loss 1.218 (0.666)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [35][60/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 4.694 (4.694)\tcontractive loss 0.092 (0.664)\tclassification loss 4.602 (4.604)\t\n",
      "Train: [35][70/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 5.850 (5.850)\tcontractive loss 1.247 (0.620)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [35][80/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 6.082 (6.082)\tcontractive loss 1.478 (0.629)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [35][90/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 4.721 (4.721)\tcontractive loss 0.118 (0.708)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [35][100/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.448 (5.448)\tcontractive loss 0.843 (0.714)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [35][110/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 5.801 (5.801)\tcontractive loss 1.197 (0.705)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [35][120/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 4.659 (4.659)\tcontractive loss 0.054 (0.702)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [35][130/145]\tBatch-Time: 0.319 (0.316)\tTotal loss 4.812 (4.812)\tcontractive loss 0.207 (0.693)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [35][140/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 6.824 (6.824)\tcontractive loss 2.221 (0.685)\tclassification loss 4.603 (4.604)\t\n",
      "epoch 35, total time 45.87\n",
      "Train: [36][10/145]\tBatch-Time: 0.317 (0.305)\tTotal loss 5.672 (5.672)\tcontractive loss 1.068 (0.719)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [36][20/145]\tBatch-Time: 0.316 (0.311)\tTotal loss 4.828 (4.828)\tcontractive loss 0.223 (0.534)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [36][30/145]\tBatch-Time: 0.317 (0.313)\tTotal loss 5.775 (5.775)\tcontractive loss 1.171 (0.634)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [36][40/145]\tBatch-Time: 0.317 (0.314)\tTotal loss 5.441 (5.441)\tcontractive loss 0.838 (0.658)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [36][50/145]\tBatch-Time: 0.316 (0.315)\tTotal loss 4.704 (4.704)\tcontractive loss 0.099 (0.667)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [36][60/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 5.429 (5.429)\tcontractive loss 0.826 (0.668)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [36][70/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 4.798 (4.798)\tcontractive loss 0.193 (0.650)\tclassification loss 4.606 (4.604)\t\n",
      "Train: [36][80/145]\tBatch-Time: 0.320 (0.315)\tTotal loss 4.709 (4.709)\tcontractive loss 0.104 (0.661)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [36][90/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 4.688 (4.688)\tcontractive loss 0.083 (0.664)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [36][100/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.803 (5.803)\tcontractive loss 1.198 (0.649)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [36][110/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.612 (5.612)\tcontractive loss 1.007 (0.670)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [36][120/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.671 (4.671)\tcontractive loss 0.066 (0.672)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [36][130/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.737 (5.737)\tcontractive loss 1.133 (0.669)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [36][140/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.688 (5.688)\tcontractive loss 1.083 (0.663)\tclassification loss 4.605 (4.604)\t\n",
      "epoch 36, total time 45.83\n",
      "Train: [37][10/145]\tBatch-Time: 0.317 (0.307)\tTotal loss 4.657 (4.657)\tcontractive loss 0.054 (0.420)\tclassification loss 4.602 (4.604)\t\n",
      "Train: [37][20/145]\tBatch-Time: 0.317 (0.312)\tTotal loss 5.594 (5.594)\tcontractive loss 0.990 (0.646)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [37][30/145]\tBatch-Time: 0.318 (0.314)\tTotal loss 4.631 (4.631)\tcontractive loss 0.026 (0.640)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [37][40/145]\tBatch-Time: 0.317 (0.314)\tTotal loss 5.018 (5.018)\tcontractive loss 0.413 (0.650)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [37][50/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 5.724 (5.724)\tcontractive loss 1.120 (0.645)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [37][60/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 5.027 (5.027)\tcontractive loss 0.422 (0.616)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [37][70/145]\tBatch-Time: 0.316 (0.315)\tTotal loss 4.805 (4.805)\tcontractive loss 0.201 (0.603)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [37][80/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 6.695 (6.695)\tcontractive loss 2.091 (0.623)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [37][90/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.816 (4.816)\tcontractive loss 0.211 (0.643)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [37][100/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.633 (4.633)\tcontractive loss 0.032 (0.630)\tclassification loss 4.601 (4.604)\t\n",
      "Train: [37][110/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 4.724 (4.724)\tcontractive loss 0.119 (0.651)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [37][120/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.881 (5.881)\tcontractive loss 1.278 (0.658)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [37][130/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.634 (4.634)\tcontractive loss 0.028 (0.647)\tclassification loss 4.606 (4.604)\t\n",
      "Train: [37][140/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.873 (4.873)\tcontractive loss 0.268 (0.669)\tclassification loss 4.605 (4.604)\t\n",
      "epoch 37, total time 45.85\n",
      "Train: [38][10/145]\tBatch-Time: 0.317 (0.305)\tTotal loss 4.643 (4.643)\tcontractive loss 0.039 (0.494)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [38][20/145]\tBatch-Time: 0.317 (0.311)\tTotal loss 5.982 (5.982)\tcontractive loss 1.377 (0.917)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [38][30/145]\tBatch-Time: 0.317 (0.313)\tTotal loss 6.813 (6.813)\tcontractive loss 2.209 (0.925)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [38][40/145]\tBatch-Time: 0.317 (0.314)\tTotal loss 4.755 (4.755)\tcontractive loss 0.152 (0.848)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [38][50/145]\tBatch-Time: 0.316 (0.314)\tTotal loss 4.948 (4.948)\tcontractive loss 0.344 (0.796)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [38][60/145]\tBatch-Time: 0.320 (0.315)\tTotal loss 5.746 (5.746)\tcontractive loss 1.143 (0.787)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [38][70/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 5.756 (5.756)\tcontractive loss 1.153 (0.790)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [38][80/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 5.431 (5.431)\tcontractive loss 0.828 (0.777)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [38][90/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 5.676 (5.676)\tcontractive loss 1.072 (0.761)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [38][100/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 6.150 (6.150)\tcontractive loss 1.546 (0.784)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [38][110/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 6.345 (6.345)\tcontractive loss 1.740 (0.768)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [38][120/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 6.131 (6.131)\tcontractive loss 1.529 (0.779)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [38][130/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 5.401 (5.401)\tcontractive loss 0.796 (0.780)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [38][140/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 5.346 (5.346)\tcontractive loss 0.741 (0.766)\tclassification loss 4.605 (4.604)\t\n",
      "epoch 38, total time 45.82\n",
      "Train: [39][10/145]\tBatch-Time: 0.320 (0.307)\tTotal loss 5.935 (5.935)\tcontractive loss 1.330 (1.376)\tclassification loss 4.605 (4.605)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [39][20/145]\tBatch-Time: 0.317 (0.312)\tTotal loss 5.987 (5.987)\tcontractive loss 1.382 (1.187)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [39][30/145]\tBatch-Time: 0.316 (0.314)\tTotal loss 5.666 (5.666)\tcontractive loss 1.062 (1.194)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [39][40/145]\tBatch-Time: 0.316 (0.314)\tTotal loss 5.611 (5.611)\tcontractive loss 1.005 (1.138)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [39][50/145]\tBatch-Time: 0.316 (0.315)\tTotal loss 5.743 (5.743)\tcontractive loss 1.137 (1.121)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [39][60/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 5.534 (5.534)\tcontractive loss 0.928 (1.053)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [39][70/145]\tBatch-Time: 0.316 (0.315)\tTotal loss 7.051 (7.051)\tcontractive loss 2.448 (1.011)\tclassification loss 4.603 (4.605)\t\n",
      "Train: [39][80/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 4.965 (4.965)\tcontractive loss 0.359 (0.991)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [39][90/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 5.280 (5.280)\tcontractive loss 0.676 (0.967)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [39][100/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 6.090 (6.090)\tcontractive loss 1.487 (0.966)\tclassification loss 4.603 (4.605)\t\n",
      "Train: [39][110/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 6.263 (6.263)\tcontractive loss 1.662 (0.945)\tclassification loss 4.602 (4.604)\t\n",
      "Train: [39][120/145]\tBatch-Time: 0.320 (0.316)\tTotal loss 5.343 (5.343)\tcontractive loss 0.739 (0.912)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [39][130/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.848 (5.848)\tcontractive loss 1.243 (0.917)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [39][140/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.814 (5.814)\tcontractive loss 1.209 (0.894)\tclassification loss 4.605 (4.604)\t\n",
      "epoch 39, total time 45.84\n",
      "Train: [40][10/145]\tBatch-Time: 0.317 (0.306)\tTotal loss 5.678 (5.678)\tcontractive loss 1.074 (0.704)\tclassification loss 4.603 (4.605)\t\n",
      "Train: [40][20/145]\tBatch-Time: 0.316 (0.311)\tTotal loss 5.843 (5.843)\tcontractive loss 1.237 (0.718)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [40][30/145]\tBatch-Time: 0.336 (0.319)\tTotal loss 6.187 (6.187)\tcontractive loss 1.584 (0.859)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [40][40/145]\tBatch-Time: 0.318 (0.321)\tTotal loss 7.003 (7.003)\tcontractive loss 2.399 (0.771)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [40][50/145]\tBatch-Time: 0.316 (0.324)\tTotal loss 4.761 (4.761)\tcontractive loss 0.157 (0.769)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [40][60/145]\tBatch-Time: 0.316 (0.323)\tTotal loss 6.001 (6.001)\tcontractive loss 1.396 (0.760)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [40][70/145]\tBatch-Time: 0.317 (0.322)\tTotal loss 5.535 (5.535)\tcontractive loss 0.933 (0.750)\tclassification loss 4.602 (4.605)\t\n",
      "Train: [40][80/145]\tBatch-Time: 0.316 (0.321)\tTotal loss 7.909 (7.909)\tcontractive loss 3.305 (0.785)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [40][90/145]\tBatch-Time: 0.317 (0.321)\tTotal loss 5.513 (5.513)\tcontractive loss 0.908 (0.817)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [40][100/145]\tBatch-Time: 0.317 (0.320)\tTotal loss 5.180 (5.180)\tcontractive loss 0.575 (0.805)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [40][110/145]\tBatch-Time: 0.316 (0.320)\tTotal loss 4.823 (4.823)\tcontractive loss 0.221 (0.796)\tclassification loss 4.602 (4.605)\t\n",
      "Train: [40][120/145]\tBatch-Time: 0.317 (0.320)\tTotal loss 5.214 (5.214)\tcontractive loss 0.610 (0.818)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [40][130/145]\tBatch-Time: 0.316 (0.320)\tTotal loss 4.659 (4.659)\tcontractive loss 0.053 (0.790)\tclassification loss 4.606 (4.604)\t\n",
      "Train: [40][140/145]\tBatch-Time: 0.317 (0.320)\tTotal loss 5.083 (5.083)\tcontractive loss 0.478 (0.775)\tclassification loss 4.606 (4.604)\t\n",
      "epoch 40, total time 46.32\n",
      "Train: [41][10/145]\tBatch-Time: 0.317 (0.304)\tTotal loss 4.629 (4.629)\tcontractive loss 0.024 (0.510)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [41][20/145]\tBatch-Time: 0.316 (0.311)\tTotal loss 4.649 (4.649)\tcontractive loss 0.045 (0.588)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [41][30/145]\tBatch-Time: 0.317 (0.313)\tTotal loss 4.739 (4.739)\tcontractive loss 0.133 (0.569)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [41][40/145]\tBatch-Time: 0.316 (0.314)\tTotal loss 4.967 (4.967)\tcontractive loss 0.362 (0.595)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [41][50/145]\tBatch-Time: 0.337 (0.315)\tTotal loss 5.063 (5.063)\tcontractive loss 0.458 (0.572)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [41][60/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 6.409 (6.409)\tcontractive loss 1.803 (0.608)\tclassification loss 4.606 (4.605)\t\n",
      "Train: [41][70/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 6.235 (6.235)\tcontractive loss 1.630 (0.638)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [41][80/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 6.709 (6.709)\tcontractive loss 2.103 (0.681)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [41][90/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.884 (5.884)\tcontractive loss 1.280 (0.689)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [41][100/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.675 (5.675)\tcontractive loss 1.070 (0.681)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [41][110/145]\tBatch-Time: 0.316 (0.317)\tTotal loss 5.070 (5.070)\tcontractive loss 0.465 (0.666)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [41][120/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 5.718 (5.718)\tcontractive loss 1.113 (0.675)\tclassification loss 4.605 (4.605)\t\n",
      "Train: [41][130/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 6.622 (6.622)\tcontractive loss 2.019 (0.668)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [41][140/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 5.819 (5.819)\tcontractive loss 1.214 (0.690)\tclassification loss 4.604 (4.604)\t\n",
      "epoch 41, total time 45.96\n",
      "Train: [42][10/145]\tBatch-Time: 0.317 (0.302)\tTotal loss 5.911 (5.911)\tcontractive loss 1.307 (0.400)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [42][20/145]\tBatch-Time: 0.317 (0.309)\tTotal loss 5.339 (5.339)\tcontractive loss 0.735 (0.687)\tclassification loss 4.603 (4.605)\t\n",
      "Train: [42][30/145]\tBatch-Time: 0.317 (0.312)\tTotal loss 4.684 (4.684)\tcontractive loss 0.081 (0.585)\tclassification loss 4.603 (4.605)\t\n",
      "Train: [42][40/145]\tBatch-Time: 0.316 (0.313)\tTotal loss 5.654 (5.654)\tcontractive loss 1.050 (0.613)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [42][50/145]\tBatch-Time: 0.317 (0.314)\tTotal loss 4.749 (4.749)\tcontractive loss 0.144 (0.736)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [42][60/145]\tBatch-Time: 0.316 (0.315)\tTotal loss 4.936 (4.936)\tcontractive loss 0.330 (0.724)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [42][70/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 4.773 (4.773)\tcontractive loss 0.169 (0.705)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [42][80/145]\tBatch-Time: 0.320 (0.315)\tTotal loss 5.769 (5.769)\tcontractive loss 1.165 (0.698)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [42][90/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.741 (4.741)\tcontractive loss 0.134 (0.687)\tclassification loss 4.606 (4.604)\t\n",
      "Train: [42][100/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 7.633 (7.633)\tcontractive loss 3.029 (0.670)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [42][110/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.028 (5.028)\tcontractive loss 0.423 (0.702)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [42][120/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 5.598 (5.598)\tcontractive loss 0.994 (0.734)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [42][130/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.672 (4.672)\tcontractive loss 0.068 (0.720)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [42][140/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.844 (4.844)\tcontractive loss 0.239 (0.706)\tclassification loss 4.605 (4.604)\t\n",
      "epoch 42, total time 45.85\n",
      "Train: [43][10/145]\tBatch-Time: 0.317 (0.305)\tTotal loss 5.106 (5.106)\tcontractive loss 0.503 (0.729)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [43][20/145]\tBatch-Time: 0.317 (0.311)\tTotal loss 5.528 (5.528)\tcontractive loss 0.924 (0.742)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [43][30/145]\tBatch-Time: 0.320 (0.313)\tTotal loss 4.778 (4.778)\tcontractive loss 0.175 (0.722)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [43][40/145]\tBatch-Time: 0.316 (0.314)\tTotal loss 4.668 (4.668)\tcontractive loss 0.064 (0.621)\tclassification loss 4.604 (4.605)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [43][50/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 5.396 (5.396)\tcontractive loss 0.792 (0.577)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [43][60/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 6.122 (6.122)\tcontractive loss 1.517 (0.616)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [43][70/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 6.390 (6.390)\tcontractive loss 1.787 (0.676)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [43][80/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.673 (4.673)\tcontractive loss 0.067 (0.670)\tclassification loss 4.606 (4.604)\t\n",
      "Train: [43][90/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 6.630 (6.630)\tcontractive loss 2.024 (0.662)\tclassification loss 4.606 (4.604)\t\n",
      "Train: [43][100/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.628 (4.628)\tcontractive loss 0.024 (0.631)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [43][110/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.406 (5.406)\tcontractive loss 0.801 (0.629)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [43][120/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 5.864 (5.864)\tcontractive loss 1.260 (0.673)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [43][130/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.662 (4.662)\tcontractive loss 0.056 (0.646)\tclassification loss 4.606 (4.604)\t\n",
      "Train: [43][140/145]\tBatch-Time: 0.320 (0.316)\tTotal loss 5.602 (5.602)\tcontractive loss 0.998 (0.654)\tclassification loss 4.604 (4.604)\t\n",
      "epoch 43, total time 45.87\n",
      "Train: [44][10/145]\tBatch-Time: 0.317 (0.311)\tTotal loss 5.720 (5.720)\tcontractive loss 1.116 (0.935)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [44][20/145]\tBatch-Time: 0.317 (0.314)\tTotal loss 5.753 (5.753)\tcontractive loss 1.148 (0.845)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [44][30/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 5.950 (5.950)\tcontractive loss 1.347 (0.765)\tclassification loss 4.602 (4.604)\t\n",
      "Train: [44][40/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 6.336 (6.336)\tcontractive loss 1.731 (0.746)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [44][50/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 4.636 (4.636)\tcontractive loss 0.032 (0.718)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [44][60/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.051 (5.051)\tcontractive loss 0.448 (0.754)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [44][70/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.153 (5.153)\tcontractive loss 0.548 (0.741)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [44][80/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 4.688 (4.688)\tcontractive loss 0.083 (0.739)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [44][90/145]\tBatch-Time: 0.318 (0.316)\tTotal loss 4.881 (4.881)\tcontractive loss 0.275 (0.730)\tclassification loss 4.606 (4.604)\t\n",
      "Train: [44][100/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.572 (5.572)\tcontractive loss 0.970 (0.712)\tclassification loss 4.602 (4.604)\t\n",
      "Train: [44][110/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 6.239 (6.239)\tcontractive loss 1.635 (0.708)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [44][120/145]\tBatch-Time: 0.316 (0.317)\tTotal loss 4.808 (4.808)\tcontractive loss 0.205 (0.682)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [44][130/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 6.153 (6.153)\tcontractive loss 1.549 (0.698)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [44][140/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 4.970 (4.970)\tcontractive loss 0.365 (0.705)\tclassification loss 4.604 (4.604)\t\n",
      "epoch 44, total time 45.92\n",
      "Train: [45][10/145]\tBatch-Time: 0.316 (0.309)\tTotal loss 4.779 (4.779)\tcontractive loss 0.175 (0.462)\tclassification loss 4.604 (4.605)\t\n",
      "Train: [45][20/145]\tBatch-Time: 0.317 (0.313)\tTotal loss 5.423 (5.423)\tcontractive loss 0.819 (0.445)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [45][30/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 4.956 (4.956)\tcontractive loss 0.353 (0.462)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [45][40/145]\tBatch-Time: 0.334 (0.319)\tTotal loss 4.782 (4.782)\tcontractive loss 0.176 (0.522)\tclassification loss 4.606 (4.604)\t\n",
      "Train: [45][50/145]\tBatch-Time: 0.320 (0.320)\tTotal loss 5.821 (5.821)\tcontractive loss 1.216 (0.555)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [45][60/145]\tBatch-Time: 0.324 (0.320)\tTotal loss 6.307 (6.307)\tcontractive loss 1.705 (0.542)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [45][70/145]\tBatch-Time: 0.324 (0.321)\tTotal loss 6.533 (6.533)\tcontractive loss 1.930 (0.505)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [45][80/145]\tBatch-Time: 0.324 (0.321)\tTotal loss 6.301 (6.301)\tcontractive loss 1.696 (0.566)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [45][90/145]\tBatch-Time: 0.326 (0.321)\tTotal loss 4.679 (4.679)\tcontractive loss 0.074 (0.558)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [45][100/145]\tBatch-Time: 0.317 (0.321)\tTotal loss 6.699 (6.699)\tcontractive loss 2.094 (0.631)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [45][110/145]\tBatch-Time: 0.319 (0.322)\tTotal loss 5.129 (5.129)\tcontractive loss 0.524 (0.646)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [45][120/145]\tBatch-Time: 0.317 (0.322)\tTotal loss 4.854 (4.854)\tcontractive loss 0.251 (0.654)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [45][130/145]\tBatch-Time: 0.343 (0.323)\tTotal loss 5.367 (5.367)\tcontractive loss 0.765 (0.665)\tclassification loss 4.602 (4.604)\t\n",
      "Train: [45][140/145]\tBatch-Time: 0.364 (0.324)\tTotal loss 4.650 (4.650)\tcontractive loss 0.047 (0.668)\tclassification loss 4.603 (4.604)\t\n",
      "epoch 45, total time 47.23\n",
      "Train: [46][10/145]\tBatch-Time: 0.347 (0.356)\tTotal loss 4.780 (4.780)\tcontractive loss 0.175 (0.304)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [46][20/145]\tBatch-Time: 0.348 (0.354)\tTotal loss 4.628 (4.628)\tcontractive loss 0.024 (0.529)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [46][30/145]\tBatch-Time: 0.368 (0.354)\tTotal loss 4.674 (4.674)\tcontractive loss 0.069 (0.698)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [46][40/145]\tBatch-Time: 0.367 (0.357)\tTotal loss 5.491 (5.491)\tcontractive loss 0.886 (0.728)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [46][50/145]\tBatch-Time: 0.367 (0.359)\tTotal loss 4.923 (4.923)\tcontractive loss 0.317 (0.728)\tclassification loss 4.606 (4.604)\t\n",
      "Train: [46][60/145]\tBatch-Time: 0.364 (0.360)\tTotal loss 5.499 (5.499)\tcontractive loss 0.896 (0.695)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [46][70/145]\tBatch-Time: 0.365 (0.361)\tTotal loss 5.999 (5.999)\tcontractive loss 1.395 (0.716)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [46][80/145]\tBatch-Time: 0.364 (0.361)\tTotal loss 6.194 (6.194)\tcontractive loss 1.590 (0.740)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [46][90/145]\tBatch-Time: 0.366 (0.362)\tTotal loss 5.865 (5.865)\tcontractive loss 1.263 (0.737)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [46][100/145]\tBatch-Time: 0.316 (0.362)\tTotal loss 4.928 (4.928)\tcontractive loss 0.322 (0.713)\tclassification loss 4.606 (4.604)\t\n",
      "Train: [46][110/145]\tBatch-Time: 0.323 (0.358)\tTotal loss 5.823 (5.823)\tcontractive loss 1.218 (0.692)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [46][120/145]\tBatch-Time: 0.318 (0.355)\tTotal loss 5.035 (5.035)\tcontractive loss 0.432 (0.685)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [46][130/145]\tBatch-Time: 0.319 (0.352)\tTotal loss 5.303 (5.303)\tcontractive loss 0.702 (0.705)\tclassification loss 4.602 (4.604)\t\n",
      "Train: [46][140/145]\tBatch-Time: 0.317 (0.350)\tTotal loss 5.118 (5.118)\tcontractive loss 0.515 (0.698)\tclassification loss 4.604 (4.604)\t\n",
      "epoch 46, total time 50.54\n",
      "Train: [47][10/145]\tBatch-Time: 0.319 (0.305)\tTotal loss 5.529 (5.529)\tcontractive loss 0.924 (0.495)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [47][20/145]\tBatch-Time: 0.317 (0.311)\tTotal loss 5.943 (5.943)\tcontractive loss 1.340 (0.431)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [47][30/145]\tBatch-Time: 0.317 (0.313)\tTotal loss 5.338 (5.338)\tcontractive loss 0.736 (0.509)\tclassification loss 4.602 (4.604)\t\n",
      "Train: [47][40/145]\tBatch-Time: 0.318 (0.315)\tTotal loss 4.654 (4.654)\tcontractive loss 0.049 (0.548)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [47][50/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.685 (5.685)\tcontractive loss 1.081 (0.569)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [47][60/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 6.002 (6.002)\tcontractive loss 1.400 (0.595)\tclassification loss 4.602 (4.604)\t\n",
      "Train: [47][70/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 6.382 (6.382)\tcontractive loss 1.778 (0.616)\tclassification loss 4.604 (4.604)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [47][80/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 5.584 (5.584)\tcontractive loss 0.982 (0.630)\tclassification loss 4.602 (4.604)\t\n",
      "Train: [47][90/145]\tBatch-Time: 0.316 (0.316)\tTotal loss 4.646 (4.646)\tcontractive loss 0.040 (0.659)\tclassification loss 4.606 (4.604)\t\n",
      "Train: [47][100/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 4.680 (4.680)\tcontractive loss 0.076 (0.647)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [47][110/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 4.656 (4.656)\tcontractive loss 0.051 (0.658)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [47][120/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 4.648 (4.648)\tcontractive loss 0.042 (0.649)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [47][130/145]\tBatch-Time: 0.317 (0.317)\tTotal loss 4.951 (4.951)\tcontractive loss 0.347 (0.654)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [47][140/145]\tBatch-Time: 0.318 (0.317)\tTotal loss 7.205 (7.205)\tcontractive loss 2.600 (0.649)\tclassification loss 4.605 (4.604)\t\n",
      "epoch 47, total time 45.94\n",
      "Train: [48][10/145]\tBatch-Time: 0.317 (0.303)\tTotal loss 4.622 (4.622)\tcontractive loss 0.018 (0.528)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [48][20/145]\tBatch-Time: 0.317 (0.310)\tTotal loss 4.778 (4.778)\tcontractive loss 0.174 (0.578)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [48][30/145]\tBatch-Time: 0.318 (0.312)\tTotal loss 4.803 (4.803)\tcontractive loss 0.198 (0.533)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [48][40/145]\tBatch-Time: 0.317 (0.313)\tTotal loss 4.612 (4.612)\tcontractive loss 0.006 (0.479)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [48][50/145]\tBatch-Time: 0.317 (0.314)\tTotal loss 4.612 (4.612)\tcontractive loss 0.008 (0.480)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [48][60/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 4.613 (4.613)\tcontractive loss 0.006 (0.487)\tclassification loss 4.606 (4.604)\t\n",
      "Train: [48][70/145]\tBatch-Time: 0.316 (0.315)\tTotal loss 4.611 (4.611)\tcontractive loss 0.006 (0.556)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [48][80/145]\tBatch-Time: 0.317 (0.315)\tTotal loss 5.743 (5.743)\tcontractive loss 1.141 (0.578)\tclassification loss 4.602 (4.604)\t\n",
      "Train: [48][90/145]\tBatch-Time: 0.316 (0.315)\tTotal loss 5.952 (5.952)\tcontractive loss 1.346 (0.626)\tclassification loss 4.606 (4.604)\t\n",
      "Train: [48][100/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 4.700 (4.700)\tcontractive loss 0.095 (0.648)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [48][110/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 6.248 (6.248)\tcontractive loss 1.645 (0.659)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [48][120/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.123 (5.123)\tcontractive loss 0.520 (0.687)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [48][130/145]\tBatch-Time: 0.317 (0.316)\tTotal loss 5.738 (5.738)\tcontractive loss 1.137 (0.684)\tclassification loss 4.601 (4.604)\t\n",
      "Train: [48][140/145]\tBatch-Time: 0.320 (0.316)\tTotal loss 5.451 (5.451)\tcontractive loss 0.845 (0.695)\tclassification loss 4.606 (4.604)\t\n",
      "epoch 48, total time 45.84\n",
      "Train: [49][10/145]\tBatch-Time: 0.360 (0.338)\tTotal loss 5.663 (5.663)\tcontractive loss 1.059 (0.515)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [49][20/145]\tBatch-Time: 0.319 (0.330)\tTotal loss 4.674 (4.674)\tcontractive loss 0.072 (0.558)\tclassification loss 4.602 (4.604)\t\n",
      "Train: [49][30/145]\tBatch-Time: 0.319 (0.326)\tTotal loss 4.776 (4.776)\tcontractive loss 0.172 (0.669)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [49][40/145]\tBatch-Time: 0.316 (0.325)\tTotal loss 5.574 (5.574)\tcontractive loss 0.972 (0.665)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [49][50/145]\tBatch-Time: 0.319 (0.324)\tTotal loss 4.990 (4.990)\tcontractive loss 0.387 (0.656)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [49][60/145]\tBatch-Time: 0.336 (0.326)\tTotal loss 5.777 (5.777)\tcontractive loss 1.172 (0.640)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [49][70/145]\tBatch-Time: 0.317 (0.326)\tTotal loss 4.698 (4.698)\tcontractive loss 0.094 (0.593)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [49][80/145]\tBatch-Time: 0.333 (0.326)\tTotal loss 4.640 (4.640)\tcontractive loss 0.036 (0.573)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [49][90/145]\tBatch-Time: 0.321 (0.325)\tTotal loss 4.819 (4.819)\tcontractive loss 0.214 (0.532)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [49][100/145]\tBatch-Time: 0.340 (0.325)\tTotal loss 7.870 (7.870)\tcontractive loss 3.265 (0.570)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [49][110/145]\tBatch-Time: 0.326 (0.326)\tTotal loss 5.758 (5.758)\tcontractive loss 1.153 (0.586)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [49][120/145]\tBatch-Time: 0.317 (0.325)\tTotal loss 4.674 (4.674)\tcontractive loss 0.068 (0.605)\tclassification loss 4.606 (4.604)\t\n",
      "Train: [49][130/145]\tBatch-Time: 0.317 (0.325)\tTotal loss 4.770 (4.770)\tcontractive loss 0.166 (0.638)\tclassification loss 4.603 (4.604)\t\n",
      "Train: [49][140/145]\tBatch-Time: 0.331 (0.325)\tTotal loss 5.095 (5.095)\tcontractive loss 0.491 (0.654)\tclassification loss 4.603 (4.604)\t\n",
      "epoch 49, total time 47.08\n",
      "Train: [50][10/145]\tBatch-Time: 0.324 (0.322)\tTotal loss 4.995 (4.995)\tcontractive loss 0.392 (0.931)\tclassification loss 4.603 (4.605)\t\n",
      "Train: [50][20/145]\tBatch-Time: 0.320 (0.322)\tTotal loss 4.828 (4.828)\tcontractive loss 0.221 (0.917)\tclassification loss 4.607 (4.605)\t\n",
      "Train: [50][30/145]\tBatch-Time: 0.318 (0.322)\tTotal loss 5.703 (5.703)\tcontractive loss 1.099 (0.837)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [50][40/145]\tBatch-Time: 0.317 (0.322)\tTotal loss 5.052 (5.052)\tcontractive loss 0.446 (0.779)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [50][50/145]\tBatch-Time: 0.340 (0.322)\tTotal loss 5.543 (5.543)\tcontractive loss 0.939 (0.765)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [50][60/145]\tBatch-Time: 0.317 (0.322)\tTotal loss 4.636 (4.636)\tcontractive loss 0.032 (0.728)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [50][70/145]\tBatch-Time: 0.321 (0.323)\tTotal loss 6.017 (6.017)\tcontractive loss 1.412 (0.706)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [50][80/145]\tBatch-Time: 0.316 (0.322)\tTotal loss 5.668 (5.668)\tcontractive loss 1.063 (0.699)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [50][90/145]\tBatch-Time: 0.329 (0.322)\tTotal loss 5.118 (5.118)\tcontractive loss 0.513 (0.679)\tclassification loss 4.605 (4.604)\t\n",
      "Train: [50][100/145]\tBatch-Time: 0.337 (0.322)\tTotal loss 5.244 (5.244)\tcontractive loss 0.642 (0.661)\tclassification loss 4.602 (4.604)\t\n",
      "Train: [50][110/145]\tBatch-Time: 0.318 (0.323)\tTotal loss 4.654 (4.654)\tcontractive loss 0.052 (0.660)\tclassification loss 4.602 (4.604)\t\n",
      "Train: [50][120/145]\tBatch-Time: 0.317 (0.323)\tTotal loss 4.623 (4.623)\tcontractive loss 0.019 (0.671)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [50][130/145]\tBatch-Time: 0.319 (0.323)\tTotal loss 5.398 (5.398)\tcontractive loss 0.795 (0.698)\tclassification loss 4.604 (4.604)\t\n",
      "Train: [50][140/145]\tBatch-Time: 0.333 (0.323)\tTotal loss 5.772 (5.772)\tcontractive loss 1.169 (0.689)\tclassification loss 4.603 (4.604)\t\n",
      "epoch 50, total time 46.79\n",
      "==> Saving...\n"
     ]
    }
   ],
   "source": [
    "opt = parse_option()\n",
    "\n",
    "# build data loader\n",
    "train_loader = set_loader(opt)\n",
    "\n",
    "# build model and criterion\n",
    "model, criterion, clss_criterion= set_model(opt)\n",
    "\n",
    "# build optimizer\n",
    "optimizer = set_optimizer(opt, model)\n",
    "\n",
    "# tensorboard\n",
    "logger = tb_logger.Logger(logdir=opt.tb_folder, flush_secs=2)\n",
    "\n",
    "# training routine\n",
    "for epoch in range(1, opt.epochs + 1):\n",
    "    adjust_learning_rate(opt, optimizer, epoch)\n",
    "\n",
    "    # train for one epoch\n",
    "    time1 = time.time()\n",
    "    loss, class_loss = train(train_loader, model, criterion, clss_criterion, optimizer, epoch, opt)\n",
    "    time2 = time.time()\n",
    "    print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
    "\n",
    "    # tensorboard logger\n",
    "    logger.log_value('contractive loss', loss, epoch)\n",
    "    logger.log_value('classification loss', class_loss, epoch)\n",
    "    logger.log_value('learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "#         if epoch % opt.save_freq == 0:\n",
    "#             save_file = os.path.join(\n",
    "#                 opt.save_folder, 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n",
    "#             save_model(model, optimizer, opt, epoch, save_file)\n",
    "\n",
    "# save the last model\n",
    "save_file = os.path.join(opt.save_folder, 'last.pth')\n",
    "save_model(model, optimizer, opt, opt.epochs, save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for idx, (images, labels) in enumerate(train_loader):\n",
    "#     print(len(images))\n",
    "#     print(labels.shape)\n",
    "#     print(images[0])\n",
    "#     print('##############################################')\n",
    "#     print(images[1])\n",
    "#     print('##############################################')\n",
    "#     print(torch.cat([images[0], images[1]], dim=0))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            # transforms.PILToTensor(),\n",
    "            transforms.Resize(size=(128, 128)),\n",
    "            transforms.ToTensor(),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_func(img_path):\n",
    "    img = Image.open(img_path)\n",
    "    tensor = transform(img).unsqueeze(0)\n",
    "    return tensor.to('cpu') #(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(images):\n",
    "    \n",
    "    if type(images) is not torch.Tensor:\n",
    "        \n",
    "        images = np.concatenate(images)\n",
    "        images = torch.Tensor(images).to('cuda')\n",
    "    with torch.no_grad():\n",
    "        res, _ = model(images).detach().cpu().numpy()    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c5f7067dd4b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-c5f7067dd4b2>\u001b[0m in \u001b[0;36mget_all_features\u001b[0;34m(img_dir)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mbs_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mbs_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-2e5f9fcdb881>\u001b[0m in \u001b[0;36mget_features\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "default='/home/ivoks/Desktop/cowsNose'\n",
    "\n",
    "def get_all_features(img_dir):\n",
    "    features = []\n",
    "    paths = []\n",
    "    bs = []\n",
    "    bs_path = []\n",
    "    e = 0\n",
    "    for root, dirs, files in os.walk(img_dir):\n",
    "        for file in files:\n",
    "            if not file.endswith('jpg') and not file.endswith('png') and not file.endswith('jpeg'):\n",
    "                print(file + \" is passed!\")\n",
    "                continue\n",
    "            img_path = os.path.join(root, file)\n",
    "            #if Image.open(img_path).mode != 'RGB':\n",
    "            #       continue\n",
    "            img = preprocess_func(img_path)\n",
    "            if len(bs) < 8:\n",
    "                bs.append(img)\n",
    "                bs_path.append(img_path)\n",
    "            else:\n",
    "                bs_features = get_features(bs)\n",
    "                features.extend(bs_features)\n",
    "                paths.extend(bs_path)\n",
    "                bs = [img]\n",
    "                bs_path = [img_path]\n",
    "                \n",
    "            e += 1\n",
    "    bs_features = get_features(bs)\n",
    "    features.extend(bs_features)\n",
    "    paths.extend(bs_path)\n",
    "    print('extracting ' + str(e) + ' features is done!')\n",
    "    return paths, features\n",
    "\n",
    "paths, features = get_all_features(default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(img_path):\n",
    "    img = preprocess_func(img_path)\n",
    "    with torch.no_grad():\n",
    "        res = model(img)[0].detach().cpu().numpy()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar(img_path, n=10, distance='euclidean'):\n",
    "    feature = get_feature(img_path)\n",
    "    p = cdist(np.array(features), np.expand_dims(feature, axis=0), metric=distance)[:, 0]\n",
    "    # p = np.sqrt(np.sum((np.array(self.features - feature)) ** 2, axis=1))\n",
    "    group = zip(p, paths.copy())\n",
    "    res = sorted(group, key=lambda x: x[0])\n",
    "    r = res[:n]\n",
    "    return r\n",
    "\n",
    "get_most_similar('/home/ivoks/Desktop/XK16-1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_1 = '/home/ivoks/Desktop/cowsNose/9/9-0.jpeg'\n",
    "\n",
    "img = Image.open(path_1)\n",
    "images = torch.from_numpy(np.asarray(img).transpose([2,0,1])[None,:,:,:]).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(images.to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_feature(path):\n",
    "    \n",
    "#     img = Image.open(path)\n",
    "#     tensor = transform(img).unsqueeze(0).to('cuda')\n",
    "#     with torch.no_grad():\n",
    "#         res = model(tensor)[0].detach().cpu().numpy()\n",
    "        \n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_1 = \"/home/ivoks/Desktop/cowsNose/9/9-0.jpeg\"\n",
    "# path_2 = \"/home/ivoks/Desktop/cowsNose/9/9-2.jpeg\"\n",
    "\n",
    "# feature1 = get_feature(path_1)\n",
    "# feature2 = get_feature(path_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array([feature1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance='euclidean'\n",
    "\n",
    "# p = cdist(np.array([feature1]),np.expand_dims(feature2, axis=0),metric=distance)[:, 0]\n",
    "\n",
    "# p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
